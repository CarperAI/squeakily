{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# filter\n",
    "\n",
    "> This module contains all the various filtering options supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import datasets\n",
    "import gc\n",
    "import logging\n",
    "import multiprocessing\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "\n",
    "import networkit as nk\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "from datasets import Dataset, Features, Value, Sequence\n",
    "from datasketch import LeanMinHash, MinHash, MinHashLSH\n",
    "from rich.logging import RichHandler\n",
    "from squeakily.helpers import flagged_words, get_words\n",
    "from squeakily.helpers import stopwords, stopword_ratios\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(RichHandler(rich_tracebacks=True))\n",
    "logger.propagate = False\n",
    "datasets.logging.set_verbosity_error()\n",
    "# Turn off logging for datasets\n",
    "logging.getLogger(\"datasets\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from datasets import load_dataset\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "multiprocessing.set_start_method(\"fork\", force=True)\n",
    "\n",
    "zstd_cntxt = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _compress_ratio(\n",
    "    doc: str,               # document to be analyzed\n",
    "    compression_level: int = 3, # compression level to use\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Returns the ratio of the compressed document to the original document.\n",
    "    \"\"\"\n",
    "    global zstd_cntxt\n",
    "    if zstd_cntxt is None:\n",
    "        import zstandard as zstd\n",
    "        zstd_cntxt = zstd.ZstdCompressor(level=compression_level)\n",
    "    bts = doc.encode(\"utf-8\")\n",
    "    compressed_bts = zstd_cntxt.compress(bts)\n",
    "    # compressed_bts = zstd.compress(doc.encode(\"utf-8\"), compression_level)\n",
    "    ratio = len(compressed_bts) / len(bts)\n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def check_compression_ratio(\n",
    "    document,                           # document to be analyzed\n",
    "    compression_threshold: float = 0.5, # threshold for compression ratio\n",
    "    compression_level: int = 3,         # compression level to use\n",
    "    dry_run=False,                      # if True, returns the ratio of character repetition\n",
    ") -> bool: # returns True if document is below threshold\n",
    "    \"\"\"\n",
    "    Checks if the document is below the character repetition threshold.\n",
    "    \"\"\"\n",
    "    compress_ratio = _compress_ratio(\n",
    "        document, compression_level=compression_level\n",
    "    )\n",
    "    if dry_run:\n",
    "        return compress_ratio\n",
    "    else:\n",
    "        return compress_ratio <= compression_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "test_str0 = \"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\"\n",
    "test_str1 = \"This is a test string.\"\n",
    "assert check_compression_ratio(test_str0, dry_run=True) < check_compression_ratio(test_str1, dry_run=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _char_rep_ratio(\n",
    "    doc: str, # document to be analyzed\n",
    "    char_rep_len: int, # length of character repetition\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Returns the ratio of character repetitions in a document.\n",
    "    \"\"\"\n",
    "    def calc_ngrams(doc, n):\n",
    "        char_ngrams = [\n",
    "            doc[i : i + n] for i in range(len(doc) - n + 1)\n",
    "        ]\n",
    "        freq_char_ngrams = Counter(char_ngrams)\n",
    "        return freq_char_ngrams\n",
    "\n",
    "    freq_char_ngrams = calc_ngrams(\n",
    "        doc, char_rep_len\n",
    "    )\n",
    "    if len(freq_char_ngrams) == 0:\n",
    "        return 0\n",
    "    freq_char_ngrams = list(freq_char_ngrams.values())\n",
    "    freq_char_ngrams = sorted(freq_char_ngrams, reverse=True)\n",
    "    val_one = len([el for el in freq_char_ngrams if el == 1])\n",
    "    num_rep_char_ngrams = min(\n",
    "        int(np.sqrt(len(freq_char_ngrams))),\n",
    "        len(freq_char_ngrams) - val_one,\n",
    "    )\n",
    "    char_rep_ratio = sum(\n",
    "        freq_char_ngrams[:num_rep_char_ngrams]\n",
    "    ) / sum(freq_char_ngrams)\n",
    "    return char_rep_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def check_char_repetition(\n",
    "    document,                       # document to be analyzed\n",
    "    char_repetition_len=10,         # length of character repetition\n",
    "    char_repetition_threshold=0.2,  # threshold for character repetition\n",
    "    dry_run=False,                  # if True, returns the ratio of character repetition\n",
    ") -> bool: # returns True if document is below threshold\n",
    "    \"\"\"\n",
    "    Checks if the document is below the character repetition threshold.\n",
    "    \"\"\"\n",
    "    char_rep_ratio = _char_rep_ratio(\n",
    "        document, char_repetition_len\n",
    "    )\n",
    "    if dry_run:\n",
    "        return char_rep_ratio\n",
    "    else:\n",
    "        return char_rep_ratio <= char_repetition_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_str = \"aaabbbcccddd\"\n",
    "assert check_char_repetition(test_str, char_repetition_len=3, char_repetition_threshold=0.2) == True\n",
    "\n",
    "test_str = \"aaaaaaabbbcccddd\"\n",
    "assert check_char_repetition(test_str, char_repetition_len=3, char_repetition_threshold=0.2) == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _flag_word_ratio(\n",
    "    doc: str, # document to be analyzed\n",
    "    flagged_words: list, # list of flagged words\n",
    "    get_words_func: callable, # function to get words from document\n",
    ") -> float: # returns ratio of flagged words in document\n",
    "    \"\"\"\n",
    "    Returns the ratio of flagged words in a document.\n",
    "    \"\"\"\n",
    "    words = get_words_func(doc)\n",
    "    if not words:\n",
    "        return 0.\n",
    "    flagged_words_ratio = len(\n",
    "        [word for word in words if word in flagged_words]\n",
    "    ) / len(words)\n",
    "    if flagged_words_ratio > 1.0:\n",
    "        flagged_words_ratio = 1.0\n",
    "    return flagged_words_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def check_flagged_words(\n",
    "    document: str,                              # document to be analyzed\n",
    "    flagged_words: list = flagged_words[\"en\"],  # list of flagged words\n",
    "    flagged_words_threshold: float = 0.1,       # threshold for flagged words\n",
    "    get_words_func: callable = get_words,       # function to get words from document\n",
    "    dry_run: bool = False,                      # if True, returns the ratio of flagged words\n",
    ") -> bool:                                      # returns True if document is below threshold unless dry_run is True\n",
    "    \"\"\"\n",
    "    Checks if a document contains a high percentage of flagged words.\n",
    "    \"\"\"\n",
    "    cond = True\n",
    "    if flagged_words:\n",
    "        flagged_words_ratio = _flag_word_ratio(\n",
    "            document,\n",
    "            flagged_words,\n",
    "            get_words_func,\n",
    "        )\n",
    "        if dry_run:\n",
    "            return flagged_words_ratio\n",
    "\n",
    "        cond = flagged_words_ratio <= flagged_words_threshold\n",
    "    return cond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `check_flagged_words` filter function is purposefully hidden in this documentation as it would show the flagged words directly in the documentation, which might shock some people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert check_flagged_words(\"test\") == True\n",
    "assert check_flagged_words(\"bdsm\") == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def check_perplexity(\n",
    "    document, # document to be analyzed\n",
    "    perplexity_threshold=10_000, # threshold for perplexity\n",
    "    model=None, # model to calculate perplexity\n",
    "    dry_run=False, # if True, returns the perplexity of the document\n",
    ") -> bool: # returns True if document is below threshold\n",
    "    \"\"\"\n",
    "    Checks if the document is below the perplexity threshold.\n",
    "    \"\"\"\n",
    "    perplexity = model.get_perplexity(document)\n",
    "    if dry_run:\n",
    "        return perplexity\n",
    "    else:\n",
    "        return perplexity <= perplexity_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this test, you need to have kenlm and sentencepiece installed:\n",
    "`pip install https://github.com/kpu/kenlm/archive/master.zip sentencepiece`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nathan/miniconda3/envs/squeakily/lib/python3.10/site-packages/huggingface_hub/file_download.py:592: FutureWarning: `cached_download` is the legacy way to download files from the HF hub, please consider upgrading to `hf_hub_download`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#|eval: false\n",
    "from squeakily.helpers import KenlmModel\n",
    "\n",
    "model = KenlmModel.from_pretrained(\n",
    "    model_dataset=\"wikipedia\",\n",
    "    language=\"en\",\n",
    "    lower_case=True,\n",
    "    remove_accents=True,\n",
    "    normalize_numbers=True,\n",
    "    punctuation=1,\n",
    ")\n",
    "\n",
    "low_test_str = \"I am very perplexed\"\n",
    "high_test_str = \"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed ...\"\n",
    "\n",
    "assert check_perplexity(low_test_str, perplexity_threshold=1_000, model=model) == True\n",
    "assert check_perplexity(high_test_str, perplexity_threshold=1_000, model=model) == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def check_language(\n",
    "    document, # document to be analyzed\n",
    "    language=\"en\", # language to check\n",
    "    language_threshold=0.9, # threshold for language\n",
    "    model=None, # model to check language\n",
    "    dry_run=False, # if True, returns the language of the document\n",
    ") -> bool: # returns True if document is below threshold\n",
    "    \"\"\"\n",
    "    Checks if the document is below the language threshold.\n",
    "    \"\"\"\n",
    "    lang, prob = model.get_language(document)\n",
    "    if dry_run:\n",
    "        if lang == language:\n",
    "            return prob\n",
    "        else:\n",
    "            return -1.\n",
    "    else:\n",
    "        return language == lang and prob > language_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "#|eval: false\n",
    "from squeakily.helpers import FastTextLanguageDetector\n",
    "\n",
    "fasttext_model = FastTextLanguageDetector.from_pretrained()\n",
    "\n",
    "english_text = \"Hi, my name is John.\"\n",
    "spanish_text = \"Hola, me llamo Juan.\"\n",
    "chinese_text = \"你好，我叫张三。\"\n",
    "\n",
    "assert check_language(english_text, language=\"en\", language_threshold=0.85, model=fasttext_model) == True\n",
    "assert check_language(spanish_text, language=\"en\", language_threshold=0.85, model=fasttext_model) == False\n",
    "assert check_language(chinese_text, language=\"en\", language_threshold=0.85, model=fasttext_model) == False\n",
    "\n",
    "# test dry run\n",
    "assert check_language(english_text, language=\"en\", language_threshold=0.85, model=fasttext_model, dry_run=True) > 0.\n",
    "assert check_language(spanish_text, language=\"en\", language_threshold=0.85, model=fasttext_model, dry_run=True) == -1.\n",
    "assert check_language(chinese_text, language=\"es\", language_threshold=0.85, model=fasttext_model, dry_run=True) == -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def check_word_number(\n",
    "    document, # document to be analyzed\n",
    "    min_word_threshold=5, # minimum number of words\n",
    "    max_word_threshold=100, # maximum number of words\n",
    "    get_words_func=get_words, # function to get words from document\n",
    "    dry_run=False, # if True, returns the number of words in the document\n",
    ") -> bool: # returns True if document is between the minimum and maximum thresholds\n",
    "    \"\"\"\n",
    "    Checks if the document is between the minimum and maximum word thresholds.\n",
    "    \"\"\"\n",
    "    words = get_words_func(document)\n",
    "    if dry_run:\n",
    "        return len(words)\n",
    "    else:\n",
    "        return len(words) >= min_word_threshold and len(words) <= max_word_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_str = \"This is a test string.\"\n",
    "\n",
    "assert check_word_number(test_str, min_word_threshold=5, max_word_threshold=10) == True\n",
    "assert check_word_number(test_str, min_word_threshold=1, max_word_threshold=4) == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def check_stop_word_ratio(\n",
    "    document, # document to be analyzed\n",
    "    stop_word_threshold=stopword_ratios['en'], # threshold for stop words\n",
    "    stop_words=stopwords['en'], # list of stop words\n",
    "    get_words_func=get_words, # function to get words from document\n",
    "    dry_run=False, # if True, returns the ratio of stop words in the document\n",
    ") -> bool: # returns True if document is below the threshold\n",
    "\t\"\"\"\n",
    "\tChecks if the document contains a high percentage of stop words.\n",
    "\t\"\"\"\n",
    "\tcond = True\n",
    "\tif stop_words:\n",
    "\t\tstop_word_ratio = _flag_word_ratio(\n",
    "\t\t\tdocument,\n",
    "\t\t\tstop_words,\n",
    "\t\t\tget_words_func,\n",
    "\t\t)\n",
    "\t\tif dry_run:\n",
    "\t\t\treturn stop_word_ratio\n",
    "\t\telse:\n",
    "\t\t\tcond = stop_word_ratio <= stop_word_threshold\n",
    "\treturn cond\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [17], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39m# Test french stop words\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39massert\u001b[39;00m check_stop_word_ratio(\u001b[39m\"\u001b[39m\u001b[39mle\u001b[39m\u001b[39m\"\u001b[39m, stop_words\u001b[39m=\u001b[39mstopwords[\u001b[39m'\u001b[39m\u001b[39mfr\u001b[39m\u001b[39m'\u001b[39m]) \u001b[39m==\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[39massert\u001b[39;00m check_stop_word_ratio(\u001b[39m\"\u001b[39m\u001b[39mle chien est beau\u001b[39m\u001b[39m\"\u001b[39m, stop_words\u001b[39m=\u001b[39mstopwords[\u001b[39m'\u001b[39m\u001b[39mfr\u001b[39m\u001b[39m'\u001b[39m], dry_run\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39m0.25\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[39massert\u001b[39;00m check_stop_word_ratio(\u001b[39m\"\u001b[39m\u001b[39mle chien est beau\u001b[39m\u001b[39m\"\u001b[39m, stop_words\u001b[39m=\u001b[39mstopwords[\u001b[39m'\u001b[39m\u001b[39mfr\u001b[39m\u001b[39m'\u001b[39m], stop_word_threshold\u001b[39m=\u001b[39m\u001b[39m0.3\u001b[39m) \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#|eval: false\n",
    "assert check_stop_word_ratio(\"test\") == True\n",
    "assert check_stop_word_ratio(\"the\") == False\n",
    "assert check_stop_word_ratio(\"the funny llama\", stop_word_threshold=0.3) == False\n",
    "assert check_stop_word_ratio(\"the funny llama\", stop_word_threshold=0.5) == True\n",
    "# Test french stop words\n",
    "assert check_stop_word_ratio(\"le\", stop_words=stopwords['fr']) == False\n",
    "assert check_stop_word_ratio(\"le chien est beau\", stop_words=stopwords['fr'], dry_run=True) == 0.25\n",
    "assert check_stop_word_ratio(\"le chien est beau\", stop_words=stopwords['fr'], stop_word_threshold=0.3) == True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whole Dataset Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinHash Deduplication\n",
    "The following code has all been adapted from the awesome [Chenghao Mou](https://github.com/ChenghaoMou) and their work on the [BigCode repository](https://github.com/bigcode-project/bigcode-analysis/blob/main/data_analysis/near-deduplication)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "MINHASH_SEED = 115\n",
    "NON_ALPHA = re.compile(\"[^A-Za-z_0-9]\")\n",
    "\n",
    "random.seed(MINHASH_SEED)\n",
    "\n",
    "lsh: MinHashLSH = None\n",
    "dup_ids: set[int] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _hash_func(\n",
    "    idx: int, # The index of the record.\n",
    "    content: str, # The content to be hashed.\n",
    "    *,\n",
    "    num_perm: int # The number of permutations to use in the MinHash object.\n",
    ") -> dict[str, any]: # The MinHash signature and the index of the record.\n",
    "    \"\"\"\n",
    "    Embed the content of a record into a MinHash object. This function should be\n",
    "    used with multiprocessing and it scales well with the number of cores.\n",
    "    >>> result = _hash_func(0, \"Hello world!\", num_perm=128)\n",
    "    >>> result[\"__id__\"]\n",
    "    0\n",
    "    >>> result[\"__signature__\"].shape\n",
    "    (128,)\n",
    "    >>> result[\"__signature__\"].dtype\n",
    "    dtype('uint64')\n",
    "    \"\"\"\n",
    "    m = MinHash(num_perm=num_perm, seed=MINHASH_SEED)\n",
    "    m.update_batch([token.encode(\"utf-8\") for token in {t for t in NON_ALPHA.split(content) if t}])\n",
    "    return {\"__signature__\": m.hashvalues, \"__id__\": idx}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = _hash_func(0, \"Hello world!\", num_perm=128)\n",
    "assert result[\"__id__\"] == 0\n",
    "assert result[\"__signature__\"].shape == (128,)\n",
    "assert result[\"__signature__\"].dtype == np.uint64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _query_content(\n",
    "    idx: int, # The index of the record.\n",
    "    signature: np.ndarray, # The MinHash signature of the record to be queried.\n",
    "    *,\n",
    "    index: MinHashLSH # The MinHashLSH index. It is shared across all processes when using multiprocessing with fork without copy.\n",
    ") -> dict[str, any]: # The query result.\n",
    "    \"\"\"\n",
    "    Query the MinHashLSH index for the record. This function can be used with multiprocessing\n",
    "    as long as the index is shared across processes.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"__neighbors__\": [\n",
    "            dup_idx\n",
    "            for dup_idx in index.query(\n",
    "                LeanMinHash(seed=MINHASH_SEED, hashvalues=signature),\n",
    "            )\n",
    "            if dup_idx != idx  # exclude itself\n",
    "        ],\n",
    "        \"__id__\": idx,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"Hello world!\", \"Hello world\"]\n",
    "signatures = [_hash_func(i, content, num_perm=128) for i, content in enumerate(data)]\n",
    "index = MinHashLSH(threshold=0.5, num_perm=128)\n",
    "for signature in signatures:\n",
    "    index.insert(\n",
    "        signature[\"__id__\"],\n",
    "        MinHash(num_perm=128, hashvalues=signature[\"__signature__\"], seed=MINHASH_SEED)\n",
    "    )\n",
    "assert _query_content(0, signatures[0][\"__signature__\"], index=index) == {'__neighbors__': [1], '__id__': 0}\n",
    "assert _query_content(1, signatures[1][\"__signature__\"], index=index) == {'__neighbors__': [0], '__id__': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _jaccard_similarity(\n",
    "    s1: str, # The first string to compare.\n",
    "    s2: str # The second string to compare.\n",
    ") -> float: # The Jaccard similarity between the two strings.\n",
    "    \"\"\"\n",
    "    Calculate the jaccard similarity between two code snippets.\n",
    "    \"\"\"\n",
    "    tokens1 = set([t for t in NON_ALPHA.split(s1) if t.strip()])\n",
    "    tokens2 = set([t for t in NON_ALPHA.split(s2) if t.strip()])\n",
    "    return len(tokens1 & tokens2) / max(1, len(tokens1 | tokens2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert _jaccard_similarity(\"a = 1\", \"a = 2\") == 0.3333333333333333\n",
    "assert _jaccard_similarity(\"a = 1\", \"a = 1\") == 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _calculate_average_false_positive_rate(\n",
    "    clusters: list[list[int]], # The clusters of duplicate records.\n",
    "    reference_records: Dataset, # The reference records.\n",
    "    threshold: float, # The threshold to use for calculating the false positive rate.\n",
    "    column: str, # The column to use for calculating the false positive rate.\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Calculate the average false positive rate within each cluster. The false positives are defined as\n",
    "    number of examples that have a maximum jaccard similarity with any example in the cluster that is\n",
    "    less than the threshold. The false positive rate is defined as the number of false positives divided\n",
    "    by the number of examples in the cluster. The average false positive rate is defined as the average\n",
    "    of the false positive rate across all clusters given.\n",
    "    \"\"\"\n",
    "    cluster_false_positive_rates: list[float] = []\n",
    "    deltas: list[float] = []\n",
    "\n",
    "    for cluster in tqdm(clusters, desc=\"Calculating sampling false positive rate...\"):\n",
    "        num_false_positives = 0\n",
    "        ids = sorted(cluster)\n",
    "        for i, x in enumerate(ids):\n",
    "            is_false_positive = True\n",
    "            max_similarity = -float(\"inf\")\n",
    "            for j, y in enumerate(ids):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                # TODO This can be redundant but we only calculate this for a small sample\n",
    "                similarity = _jaccard_similarity(reference_records[x][column], reference_records[y][column])\n",
    "                max_similarity = max(max_similarity, similarity)\n",
    "                if max_similarity >= threshold:\n",
    "                    is_false_positive = False\n",
    "                    break\n",
    "            if is_false_positive:\n",
    "                num_false_positives += 1\n",
    "                deltas.append(threshold - max_similarity)\n",
    "        cluster_false_positive_rates.append(num_false_positives / len(ids))\n",
    "\n",
    "    logger.info(\n",
    "        f\"Average false positive rate from {len(clusters)} clusters: {np.mean(cluster_false_positive_rates):.2f}\"\n",
    "    )\n",
    "    logger.info(f\"Similarity delta stats from threshold:\")\n",
    "    logger.info(f\"-  Max : {np.max(deltas):0.2f}\")\n",
    "    logger.info(f\"-  Min : {np.min(deltas):0.2f}\")\n",
    "    logger.info(f\"-  Mean: {np.mean(deltas):0.2f}\")\n",
    "    logger.info(f\"-  Std : {np.std(deltas):0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _find_duplicate_communities(\n",
    "    records: Dataset, # The dataset that contains both `__id__` and `__neighbors__`.\n",
    "    community_detection: bool, # Whether to use community detection to find the duplicate communities, or to use the connected components.\n",
    "    report_false_positive_rate: bool = False, # Whether to report the false positive rate.\n",
    "    reference_records: Dataset = None, # The reference records. It can be an iterable or a Dataset. It is only used when `report_false_positive_rate` is True.\n",
    "    threshold: float = 0.85, # The threshold to use for calculating the false positive rate.\n",
    "    column: str = \"content\", # The column to use for calculating the false positive rate.\n",
    "    verbose: bool = False,\n",
    ") -> set[int]: # The set of duplicate ids that should be removed, leaving only one id in each community.\n",
    "    \"\"\"\n",
    "    Find the duplicate communities from the queried dataset.\n",
    "    \"\"\"\n",
    "    SAMPLE_MIN_SIZE = 10\n",
    "    SAMPLE_MAX_SIZE = 100\n",
    "    SAMPLE_SIZE = 10\n",
    "    g = nk.graph.Graph()\n",
    "    for record in tqdm(records, desc=\"Constructing graph...\"):\n",
    "        for y in record[\"__neighbors__\"]:\n",
    "            g.addEdge(record[\"__id__\"], y, addMissing=True)\n",
    "\n",
    "    to_remove: set[int] = set()\n",
    "    samples: list[list[int]] = []\n",
    "    if not community_detection:\n",
    "        cc = nk.components.ConnectedComponents(g)\n",
    "        cc.run()\n",
    "        partition = cc.getPartition()\n",
    "        components = list(cc.getComponents())\n",
    "        random.shuffle(components)\n",
    "        for component in tqdm(components, desc=\"Iterating over components...\"):\n",
    "            component = sorted(component)\n",
    "            to_remove.update(component[1:])\n",
    "            if len(samples) < SAMPLE_SIZE and SAMPLE_MAX_SIZE > len(component) >= SAMPLE_MIN_SIZE:\n",
    "                samples.append(component[:])\n",
    "    else:\n",
    "        algo = nk.community.PLM(g, refine=False)\n",
    "        algo.run()\n",
    "        partition = algo.getPartition()\n",
    "        communities = list(partition.getSubsetIds())\n",
    "        random.shuffle(communities)\n",
    "        # This can be slow if there are many communities\n",
    "        for i in tqdm(communities, desc=\"Iterating over communities...\"):\n",
    "            ids = partition.getMembers(i)\n",
    "            to_remove.update(sorted(ids)[1:])\n",
    "            if len(samples) < SAMPLE_SIZE and SAMPLE_MAX_SIZE > len(ids) >= SAMPLE_MIN_SIZE:\n",
    "                samples.append(ids)\n",
    "\n",
    "    if report_false_positive_rate and verbose:\n",
    "        _calculate_average_false_positive_rate(\n",
    "            samples,\n",
    "            reference_records,\n",
    "            threshold,\n",
    "            column,\n",
    "        )\n",
    "\n",
    "    return to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def minhash_dedup(\n",
    "    ds,                                         # The dataset to deduplicate.\n",
    "    column,                                     # The column to use for deduplication.\n",
    "    community_detection: bool = False,          # Whether to use community detection to find the duplicate communities, or to use the connected components.\n",
    "    report_false_positive_rate: bool = False,   # Whether to report the false positive rate.\n",
    "    threshold: float = 0.85,                    # The threshold to use for deduplication.\n",
    "    num_perm: int = 128,                        # The number of permutations to use for minhashing.\n",
    "    dry_run: bool = False,                      # Whether to run the deduplication in dry run mode.\n",
    ") -> Dataset:\n",
    "    \"\"\"\n",
    "    Deduplicate the dataset using minhashing as described in the paper \"Deduplicating Training Data Makes Language Models Better\".\n",
    "    \"\"\"\n",
    "    global lsh\n",
    "    global dup_ids\n",
    "\n",
    "    lsh = MinHashLSH(\n",
    "        threshold=threshold,\n",
    "        num_perm=num_perm,\n",
    "    )\n",
    "    column_names = ds.column_names\n",
    "    ds = ds.map(\n",
    "        lambda _, idx: {\"__id__\": idx},\n",
    "        with_indices=True,\n",
    "        num_proc=os.cpu_count(),\n",
    "        desc=\"Adding index...\",\n",
    "    )\n",
    "    hashed_ds = ds.map(\n",
    "        function=_hash_func,\n",
    "        fn_kwargs={\"num_perm\": num_perm},\n",
    "        input_columns=[\"__id__\", column],\n",
    "        remove_columns=column_names,\n",
    "        num_proc=os.cpu_count(),\n",
    "        desc=f\"Fingerprinting...\",\n",
    "    )\n",
    "    with lsh.insertion_session() as session:\n",
    "        for data in tqdm(hashed_ds, desc=\"Indexing signatures...\"):\n",
    "            if data[\"__id__\"] in lsh:\n",
    "                continue\n",
    "            session.insert(\n",
    "                data[\"__id__\"],\n",
    "                LeanMinHash(seed=MINHASH_SEED, hashvalues=data[\"__signature__\"]),\n",
    "                check_duplication=False,\n",
    "            )\n",
    "    \n",
    "    gc.disable()\n",
    "    gc.freeze()\n",
    "\n",
    "    conf = {\n",
    "        \"threshold\": threshold,\n",
    "        \"community_detection\": community_detection,\n",
    "        \"report_false_positive_rate\": report_false_positive_rate,\n",
    "        \"num_perm\": num_perm,\n",
    "        \"name\": ds.builder_name,\n",
    "        \"column\": column,\n",
    "    }\n",
    "    queried = hashed_ds.map(\n",
    "        lambda x, y: _query_content(x, y, index=lsh),\n",
    "        num_proc=os.cpu_count(),\n",
    "        features=Features({\n",
    "            \"__id__\": Value(dtype='int64', id=None),\n",
    "            \"__neighbors__\": Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)\n",
    "        }),\n",
    "        input_columns=[\"__id__\", \"__signature__\"],\n",
    "        remove_columns=[\"__signature__\"],\n",
    "        desc=f\"Querying...\",\n",
    "    )\n",
    "\n",
    "    del lsh\n",
    "    gc.collect()\n",
    "\n",
    "    queried = queried.filter(\n",
    "        lambda x: len(x[\"__neighbors__\"]) > 0,\n",
    "        num_proc=os.cpu_count(),\n",
    "        desc=\"Finding duplicates...\"\n",
    "    )\n",
    "    dup_ids = _find_duplicate_communities(\n",
    "        records=queried,\n",
    "        community_detection=conf[\"community_detection\"],\n",
    "        report_false_positive_rate=conf[\"report_false_positive_rate\"],\n",
    "        reference_records=ds,\n",
    "        threshold=conf[\"threshold\"],\n",
    "        column=conf[\"column\"],\n",
    "    )\n",
    "\n",
    "    del queried\n",
    "    gc.collect()\n",
    "\n",
    "    if dry_run:\n",
    "        final_data = ds.map(\n",
    "            lambda idx: {\"duplicate\": idx in dup_ids},\n",
    "            input_columns=[\"__id__\"],\n",
    "            num_proc=os.cpu_count(),\n",
    "            desc=\"Labeling duplicates...\",\n",
    "        )\n",
    "    else:\n",
    "        final_data = ds.filter(\n",
    "            lambda idx: idx not in dup_ids,\n",
    "            input_columns=[\"__id__\"],\n",
    "            num_proc=os.cpu_count(),\n",
    "            desc=\"Filtering duplicates...\",\n",
    "        )\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8405a6a9d16447d89efd387db70f5c91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Indexing signatures...:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c347adf2d598417f935b326e003f24e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Constructing graph...:   0%|          | 0/373 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45191f87ae8045f7911288237bbe471b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterating over communities...:   0%|          | 0/635 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                "
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"train\")\n",
    "dataset = dataset.select(range(1_000))\n",
    "deduped_dataset = minhash_dedup(dataset, \"text\", community_detection=True, threshold=0.85, num_perm=128)\n",
    "\n",
    "assert len(deduped_dataset) == len(dataset) - len(dup_ids)\n",
    "assert deduped_dataset.column_names == dataset.column_names + [\"__id__\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79533d4edf4c433cadce662e91f52515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Indexing signatures...:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6849a818ea764e2fb527d9712d533e45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Constructing graph...:   0%|          | 0/373 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3838419eb6943c8a066503a1eb84abc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterating over communities...:   0%|          | 0/635 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                "
     ]
    }
   ],
   "source": [
    "# test dry run\n",
    "deduped_dataset = minhash_dedup(dataset, \"text\", community_detection=True, threshold=0.85, num_perm=128, dry_run=True)\n",
    "\n",
    "assert len(deduped_dataset) == len(dataset)\n",
    "assert deduped_dataset.column_names == dataset.column_names + [\"__id__\", \"duplicate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Flower Fairies of the Spring ; Blackie , 1923 \n",
      "\n",
      " = = = Regular season = = = \n",
      "\n",
      " \" There 's Got to Be a Way \" ( 12 \" remix ) \n",
      "\n",
      " = = Early life = = \n",
      "\n",
      " = = Awards = = \n",
      "\n",
      " = = Critical reception = = \n",
      "\n",
      " = = History = = \n",
      "\n",
      " = = Service history = = \n",
      "\n",
      " = = Description = = \n",
      "\n",
      " = = Background = = \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print which records were removed\n",
    "for idx in dup_ids:\n",
    "    if dataset[idx][\"text\"] == \"\":\n",
    "        continue\n",
    "    print(dataset[idx][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "squeakily",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
