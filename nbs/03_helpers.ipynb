{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# helpers\n",
    "\n",
    "> This module contains all the various helper functions used in the other modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "import re\n",
    "import squeakily\n",
    "import unicodedata\n",
    "import urllib.request\n",
    "\n",
    "from huggingface_hub import cached_download, hf_hub_url\n",
    "from requests.exceptions import HTTPError\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-important}\n",
    "Note: This notebook contains a large collection of profane and offensive language to use as a word filter. It is not recommended for children or the highly sensitive.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# def quick_size_estimation(\n",
    "#     ds: Dataset,\n",
    "#     num_proc: int,\n",
    "#     batch_size: int,\n",
    "#     content_key:str =\"text\"\n",
    "# ) -> int:\n",
    "#     if len(ds) == 0:\n",
    "#         return 0\n",
    "#     rng = default_rng(1991)\n",
    "#     subset_size = min(10000, len(ds))\n",
    "#     indices = rng.choice(len(ds), size=subset_size, replace=False, shuffle=False)\n",
    "#     partial_ds = ds.select(indices)\n",
    "#     ratio = float(len(ds)) / float(subset_size)\n",
    "\n",
    "#     partial_ds = partial_ds.map(\n",
    "#         get_size_per_example,\n",
    "#         batched=True, \n",
    "#         num_proc=num_proc,\n",
    "#         batch_size=batch_size,\n",
    "#         input_columns=[content_key],\n",
    "#         remove_columns=partial_ds.column_names,\n",
    "#     )\n",
    "#     len_bytes = sum(partial_ds[\"bytes_len\"])\n",
    "#     return len_bytes * ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_words(\n",
    "    text: str, # the text to extract words from\n",
    ") -> list:\n",
    "    \"\"\"custom regex to extract all the words in a string\"\"\"\n",
    "    return re.findall(r'\\w+', text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Built from native speakers, with inspiration from\n",
    "# https://github.com/zacanger/profane-words\n",
    "# and\n",
    "# https://github.com/thisandagain/washyourmouthoutwithsoap/blob/develop/data/build.json\n",
    "# and\n",
    "# https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words\n",
    "\n",
    "\n",
    "english_flagged_words = [\n",
    "    \"anal\",\n",
    "    # \"bareback\", # not sure about this one\n",
    "    \"bbw\",\n",
    "    \"bdsm\",\n",
    "    \"blowjob\",\n",
    "    \"blowjobs\",\n",
    "    \"brazzers\",\n",
    "    \"bukkake\",\n",
    "    \"camgirl\",\n",
    "    \"camwhore\",\n",
    "    \"cocksucking\",\n",
    "    # \"cougar\", # not sure about this one\n",
    "    \"creampie\",\n",
    "    \"cuckold\",\n",
    "    \"cum\",\n",
    "    \"cumming\",\n",
    "    \"cums\",\n",
    "    \"cumshot\",\n",
    "    \"cumshots\",\n",
    "    \"cumslut\",\n",
    "    \"cunnilingus\",\n",
    "    \"deepthroat\",\n",
    "    \"deepthroating\",\n",
    "    \"dildo\",\n",
    "    \"dildos\",\n",
    "    \"dogging\",\n",
    "    \"doggystyle\",\n",
    "    # \"dominatrix\", # not sure about this one\n",
    "    \"erotic\",\n",
    "    \"fellatio\",\n",
    "    \"femdom\",\n",
    "    \"fingering\",\n",
    "    \"fisting\",\n",
    "    \"footjob\",\n",
    "    \"gangbang\",\n",
    "    \"handjob\",\n",
    "    \"hentai\",\n",
    "    \"horney\",\n",
    "    \"horniest\",\n",
    "    \"horny\",\n",
    "    \"jism\",\n",
    "    \"jizz\",\n",
    "    \"lolli\",\n",
    "    \"lolling\",\n",
    "    \"masterbating\",\n",
    "    \"masturbate\",\n",
    "    \"masturbating\",\n",
    "    \"masturbation\",\n",
    "    \"milf\",\n",
    "    \"orgies\",\n",
    "    \"orgy\",\n",
    "    \"pegging\",\n",
    "    \"porn\",\n",
    "    \"pornhub\",\n",
    "    \"porno\",\n",
    "    \"pornos\",\n",
    "    \"pornstar\",\n",
    "    \"pornstars\",\n",
    "    \"redtube\",\n",
    "    \"rimming\",\n",
    "    \"slutty\",\n",
    "    # \"squirting\", # not sure about this one\n",
    "    \"strapon\",\n",
    "    \"threesome\",\n",
    "    \"vibrator\",\n",
    "    \"xhamster\",\n",
    "    \"xnxx\",\n",
    "    \"xvideos\",\n",
    "    \"xxx\",\n",
    "    \"youporn\",\n",
    "]\n",
    "\n",
    "\n",
    "flagged_words = {\n",
    "    \"ar\": english_flagged_words\n",
    "    + [\n",
    "        \"إباحي\",\n",
    "        \"احتلام\",\n",
    "        \"است\",\n",
    "        \"استمناء\",\n",
    "        \"اغتصاب\",\n",
    "        \"أورغازم\",\n",
    "        \"إيروتيك\",\n",
    "        \"أير\",\n",
    "        \"بز\",\n",
    "        \"بزاز\",\n",
    "        \"بظر\",\n",
    "        \"بورن\",\n",
    "        \"بيضان\",\n",
    "        \"مص\",\n",
    "        \"ثدي\",\n",
    "        \"جماع\",\n",
    "        \"جنس\",\n",
    "        \"حلمة\",\n",
    "        \"خلاعة\",\n",
    "        \"خنثي\",\n",
    "        \"خول\",\n",
    "        \"دعارة\",\n",
    "        \"زب\",\n",
    "        \"سحاق\",\n",
    "        \"سحاقية\",\n",
    "        \"سكس\",\n",
    "        \"سيكس\",\n",
    "        \"شاذ\",\n",
    "        \"شبق\",\n",
    "        \"شرج\",\n",
    "        \"شرموطة\",\n",
    "        \"شهواني\",\n",
    "        \"شهوة\",\n",
    "        \"طيز\",\n",
    "        \"عادة السرية\",\n",
    "        \"عاهرة\",\n",
    "        \"عرص\",\n",
    "        \"فاسقة\",\n",
    "        \"فرج\",\n",
    "        \"قحبة\",\n",
    "        \"قضيب\",\n",
    "        \"كس\",\n",
    "        \"لحس\",\n",
    "        \"لعق\",\n",
    "        \"لواط\",\n",
    "        \"لوطي\",\n",
    "        \"مؤخرة\",\n",
    "        \"متناك\",\n",
    "        \"متناكة\",\n",
    "        \"مومس\",\n",
    "        \"مثير\",\n",
    "        \"مص\",\n",
    "        \"مضاجعة\",\n",
    "        \"مفلقسة\",\n",
    "        \"مني\",\n",
    "        \"مهتاج\",\n",
    "        \"نشوة\",\n",
    "        \"نكاح\",\n",
    "        \"نيك\",\n",
    "    ],\n",
    "    \"bn\": english_flagged_words\n",
    "    + [\n",
    "        \"আঙ্গুলি করা\",\n",
    "        \"আচোদা\",\n",
    "        \"খানকি\",\n",
    "        \"খানকি মাগি\",\n",
    "        \"গান্ড মারানো\",\n",
    "        \"গুদ মারানি\",\n",
    "        \"চুচুক\",\n",
    "        \"চোদ\",\n",
    "        \"চোদনা\",\n",
    "        \"চোদা\",\n",
    "        \"চোদা বোন\",\n",
    "        \"চোদাচুদি\",\n",
    "        \"জারজ\",\n",
    "        \"নাঙ্গ\",\n",
    "        \"নেংটা\",\n",
    "        \"পর্ণহাব\",\n",
    "        \"পর্ন\",\n",
    "        \"পর্নস্টার\",\n",
    "        \"পর্নোগ্রাফি\",\n",
    "        \"পোঁদ\",\n",
    "        \"পোঁদ মারানি\",\n",
    "        \"পোদ মারানি\",\n",
    "        \"বাঁড়া\",\n",
    "        \"বানচোদ\",\n",
    "        \"বেশ্যা\",\n",
    "        \"বেশ্যার ছেলে\",\n",
    "        \"বোকাচোদা\",\n",
    "        \"ভগ\",\n",
    "        \"মা চোদা\",\n",
    "        \"মাগী\",\n",
    "        \"মাদারচোদ\",\n",
    "        \"মুখে নিবি\",\n",
    "        \"মোরগ\",\n",
    "        \"রেন্ডি\",\n",
    "        \"শিশ্ন\",\n",
    "        \"স্তন\",\n",
    "        \"স্তনবৃন্ত\",\n",
    "        \"হস্তমৈথুন\",\n",
    "    ],\n",
    "    \"ca\": english_flagged_words\n",
    "    + [\n",
    "        \"cagarro\",\n",
    "        \"cagarros\",\n",
    "        \"cipote\",\n",
    "        \"cipotes\",\n",
    "        \"collons\",\n",
    "        \"colló\",\n",
    "        \"consolador\",\n",
    "        \"consoladors\",\n",
    "        \"cony\",\n",
    "        \"conys\",\n",
    "        \"corre's\",\n",
    "        \"corre't\",\n",
    "        \"corregut\",\n",
    "        \"cunillingus\",\n",
    "        \"córrer-se\",\n",
    "        \"escorreguda\",\n",
    "        \"escorregudes\",\n",
    "        \"escorregut\",\n",
    "        \"escrot\",\n",
    "        \"escrots\",\n",
    "        \"escórre's\",\n",
    "        \"escórre't\",\n",
    "        \"escórrer-se\",\n",
    "        \"mamada\",\n",
    "        \"mamadera\",\n",
    "        \"mamaderes\",\n",
    "        \"mamades\",\n",
    "        \"masturba\",\n",
    "        \"masturbacions\",\n",
    "        \"masturbació\",\n",
    "        \"masturbant\",\n",
    "        \"masturbar\",\n",
    "        \"masturbar-se\",\n",
    "        \"masturbat\",\n",
    "        \"masturbats\",\n",
    "        \"masturbes\",\n",
    "        \"orgasme\",\n",
    "        \"orgasmes\",\n",
    "        \"ou\",\n",
    "        \"ous\",\n",
    "        \"palla\",\n",
    "        \"palles\",\n",
    "        \"pornografia\",\n",
    "        \"semen\",\n",
    "        \"semens\",\n",
    "        \"verga\",\n",
    "        \"vergues\",\n",
    "        \"xxx\",\n",
    "    ],\n",
    "    \"en\": english_flagged_words,\n",
    "    \"es\": english_flagged_words\n",
    "    + [\n",
    "        \"chupar el coño\",\n",
    "        \"chupar la concha\",\n",
    "        \"chupar la polla\",\n",
    "        \"chupar la verga\",\n",
    "        \"comer el coño\",\n",
    "        \"comer la concha\",\n",
    "        \"comer la polla\",\n",
    "        \"comer la verga\",\n",
    "        \"coprofagía\",\n",
    "        \"correrse\",\n",
    "        \"cunillingus\",\n",
    "        \"fagging\",\n",
    "        \"felación\",\n",
    "        \"felching\",\n",
    "        \"follada\",\n",
    "        \"follador de culo\",\n",
    "        \"folladores\",\n",
    "        \"fudge packer\",\n",
    "        \"hacer una paja\",\n",
    "        \"hacerse una paja\",\n",
    "        \"hore\",\n",
    "        \"kock\",\n",
    "        \"macizorra\",\n",
    "        \"madre folladora\",\n",
    "        \"mamada\",\n",
    "        \"perro follador\",\n",
    "        \"pisser\",\n",
    "        \"pornografía\",\n",
    "        \"sado\",\n",
    "        \"sadomasoquismo\",\n",
    "        \"sadomasoquista\",\n",
    "        \"sexo anal\",\n",
    "        \"skank\",\n",
    "        \"smegma\",\n",
    "        \"x clasificado\",\n",
    "    ],\n",
    "    \"eu\": english_flagged_words + [],\n",
    "    \"fr\": english_flagged_words\n",
    "    + [\n",
    "        \"baiseurs\",\n",
    "        \"baiseur\",\n",
    "        \"baiseuse\",\n",
    "        \"baiseuses\",\n",
    "        \"branlette\",\n",
    "        \"branlettes\",\n",
    "        \"branleuse\",\n",
    "        \"branleuses\",\n",
    "        \"cunillingus\",\n",
    "        \"cunilingus\",\n",
    "        \"enculée\",\n",
    "        \"enculées\",\n",
    "        \"enculation\",\n",
    "        \"enculations\",\n",
    "        \"enculement\",\n",
    "        \"enculements\",\n",
    "        \"fellation\",\n",
    "        \"fellations\",\n",
    "        \"porno\",\n",
    "        \"pornos\",\n",
    "        \"pornographie\",\n",
    "        \"pornographique\",\n",
    "        \"pornographiques\",\n",
    "        \"salope\",\n",
    "        \"salopes\",\n",
    "        \"suceuse\",\n",
    "        \"suceuses\",\n",
    "        \"xxx\",\n",
    "    ],\n",
    "    \"hi\": english_flagged_words\n",
    "    + [\n",
    "        \"अंडकोश की थैली\",\n",
    "        \"एक्स रेटेड\",\n",
    "        \"ओगाज़्म\",\n",
    "        \"कामोद्दीपक चित्र\",\n",
    "        \"कालीन का चूरा\",\n",
    "        \"कून\",\n",
    "        \"कॉक\",\n",
    "        \"गेंद का थैला\",\n",
    "        \"चाकलेट का रंग\",\n",
    "        \"चूची\",\n",
    "        \"चूतड़\",\n",
    "        \"झटका बंद\",\n",
    "        \"ठगना पैकर\",\n",
    "        \"डिल्डो\",\n",
    "        \"नितंब\",\n",
    "        \"पिछाड़ी\",\n",
    "        \"पीड़न कामुक\",\n",
    "        \"पॉर्न\",\n",
    "        \"फटना\",\n",
    "        \"फूहड़\",\n",
    "        \"बट\",\n",
    "        \"बहुत मदहोश\",\n",
    "        \"बेल अंत\",\n",
    "        \"भगवान-शापित\",\n",
    "        \"भगशेफ\",\n",
    "        \"माँ कमीने\",\n",
    "        \"मुखमैथुन\",\n",
    "        \"मुर्गा चूसने वाला\",\n",
    "        \"रक्तरंजित\",\n",
    "        \"लेबिया\",\n",
    "        \"वहशी\",\n",
    "        \"वहशीता\",\n",
    "        \"वैंग\",\n",
    "        \"शिश्नमल\",\n",
    "        \"संभोग सुख\",\n",
    "        \"सह शॉट\",\n",
    "        \"सींग का बना हुआ\",\n",
    "        \"होर\",\n",
    "        \"घपा घप\",\n",
    "        \"चुदाई\",\n",
    "        \"चुदक्कड़\",\n",
    "    ],\n",
    "    \"id\": english_flagged_words\n",
    "    + [\n",
    "        \"bokep\",\n",
    "        \"coli\",\n",
    "        \"colmek\",\n",
    "        \"grepe\",\n",
    "        \"horni\",\n",
    "        \"janda\",\n",
    "        \"jembut\",\n",
    "        \"jilat memek\",\n",
    "        \"jilmek\",\n",
    "        \"kontol\",\n",
    "        \"masturbasi\",\n",
    "        \"memek\",\n",
    "        \"ngentot\",\n",
    "        \"ngewe\",\n",
    "        \"peju\",\n",
    "        \"pepek\",\n",
    "        \"pornografi\",\n",
    "        \"sange\",\n",
    "        \"sepong\",\n",
    "        \"tusbol\",\n",
    "    ],\n",
    "    \"pt\": english_flagged_words\n",
    "    + [\n",
    "        \"balalao\",\n",
    "        \"bate uma\",\n",
    "        \"beijo grego\",\n",
    "        \"boceta\",\n",
    "        \"boquete\",\n",
    "        \"buceta\",\n",
    "        \"caralho\",\n",
    "        \"chochota\",\n",
    "        \"coito\",\n",
    "        \"cona\",\n",
    "        \"consolo\",\n",
    "        \"corno\",\n",
    "        \"cu\",\n",
    "        \"dar a bunda\",\n",
    "        \"dar o rabo\",\n",
    "        \"dildo\",\n",
    "        \"dildos\",\n",
    "        \"esporrar\",\n",
    "        \"estrovenga\",\n",
    "        \"felação\",\n",
    "        \"filho da puta\",\n",
    "        \"filhos da puta\",\n",
    "        \"gozada\",\n",
    "        \"jeba\",\n",
    "        \"perereca\",\n",
    "        \"pica\",\n",
    "        \"piru\",\n",
    "        \"porno\",\n",
    "        \"pornografia\",\n",
    "        \"pornô\",\n",
    "        \"porra\",\n",
    "        \"prostituta\",\n",
    "        \"pube\",\n",
    "        \"punheta\",\n",
    "        \"punheteiro\",\n",
    "        \"putaria\",\n",
    "        \"queca\",\n",
    "        \"sexo\",\n",
    "        \"siririca\",\n",
    "        \"tesão\",\n",
    "        \"trepada\",\n",
    "        \"verga\",\n",
    "        \"vibrador\",\n",
    "        \"xana\",\n",
    "        \"xochota\",\n",
    "        \"xoxota\",\n",
    "    ],\n",
    "    \"ur\": english_flagged_words\n",
    "    + [\n",
    "        \"انگلی کرنا\",\n",
    "        \"ایکس ریٹیڈ\",\n",
    "        \"بلو جاب\",\n",
    "        \"بٹ\",\n",
    "        \"جھٹکا بند\",\n",
    "        \"دلڈو\",\n",
    "        \"رنڈی\",\n",
    "        \"سلٹ\",\n",
    "        \"سکلیرا\",\n",
    "        \"سیڈسٹ\",\n",
    "        \"سیکس بم\",\n",
    "        \"شہوانی\",\n",
    "        \"شہوت انگیز\",\n",
    "        \"فحش نگاری\",\n",
    "        \"لن\",\n",
    "        \"لنڈ\",\n",
    "        \"لنڈ چوسنے والا\",\n",
    "        \"لوڑہ\",\n",
    "        \"ماں کمینے\",\n",
    "        \"مشت زنی\",\n",
    "        \"ممے\",\n",
    "        \"مٹھ\",\n",
    "        \"مٹھی\",\n",
    "        \"ویشیا\",\n",
    "        \"پورن\",\n",
    "        \"پھدی\",\n",
    "        \"پیگنگ\",\n",
    "        \"چدائ\",\n",
    "        \"چدک\",\n",
    "        \"چوت\",\n",
    "        \"چودنا\",\n",
    "        \"چوچی\",\n",
    "        \"کسبی\",\n",
    "        \"کسنگ\",\n",
    "        \"گانڈ\",\n",
    "        \"گدا\",\n",
    "        \"ہینڈ جاب\",\n",
    "    ],\n",
    "    \"vi\": english_flagged_words\n",
    "    + [\n",
    "        \"cặc\",\n",
    "        \"lồn\",\n",
    "        \"địt\",\n",
    "        \"buồi\",\n",
    "        \"đụ\",\n",
    "        \"nứng\",\n",
    "        \"phò\",\n",
    "        \"bú lol\",\n",
    "        \"bú lon\",\n",
    "        \"bú cac\",\n",
    "        \"bú cak\",\n",
    "        \"bú cax\",\n",
    "        \"bú buoi\",\n",
    "        \"bú mút\",\n",
    "        \"húp lol\",\n",
    "        \"húp lon\",\n",
    "        \"húp sò\",\n",
    "        \"nung lol\",\n",
    "        \"nung lon\",\n",
    "        \"nug lon\",\n",
    "        \"nung cac\",\n",
    "        \"nung buoi\",\n",
    "        \"thèm dit\",\n",
    "        \"thèm lon\",\n",
    "        \"thèm lol\",\n",
    "        \"thèm buoi\",\n",
    "        \"thèm dit\",\n",
    "        \"thèm chich\",\n",
    "        \"thèm chịch\",\n",
    "        \"thèm cax\",\n",
    "        \"thèm cak\",\n",
    "        \"móc lol\",\n",
    "        \"móc lon\",\n",
    "        \"buscu\",\n",
    "        \"lol non\",\n",
    "        \"bướm non\",\n",
    "        \"vú to\",\n",
    "        \"vú bự\",\n",
    "        \"lon to\",\n",
    "        \"phim sex\",\n",
    "        \"phim xex\",\n",
    "        \"phim xxx\",\n",
    "    ],\n",
    "    \"zh\": english_flagged_words\n",
    "    + [\n",
    "        \"三级片\",\n",
    "        \"乳交\",\n",
    "        \"卖B\",\n",
    "        \"卖比\",\n",
    "        \"卖淫\",\n",
    "        \"口交\",\n",
    "        \"姣西\",\n",
    "        \"打炮\",\n",
    "        \"打飞机\",\n",
    "        \"抽插\",\n",
    "        \"招妓\",\n",
    "        \"插你\",\n",
    "        \"插死你\",\n",
    "        \"淫水\",\n",
    "        \"肉壁\",\n",
    "        \"肉棍子\",\n",
    "        \"肉缝\",\n",
    "        \"肏\",\n",
    "        \"肛交\",\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is adapted from this awesome blog post by [C Chaitanya](https://medium.com/@c.chaitanya/language-identification-in-python-using-fasttext-60359dc30ed0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class FastTextLanguageDetector:\n",
    "    def __init__(self, model_path: str = \"/tmp/lid.176.bin\"):\n",
    "        import fasttext\n",
    "        self.model_path = model_path\n",
    "        self.model = fasttext.load_model(model_path)\n",
    "\n",
    "    def get_language(self, text):\n",
    "        lines = \" \".join(text.splitlines())\n",
    "        prediction = self.model.predict(lines, k=1) # returns top 2 matching languages\n",
    "        lang, prob = prediction[0][0].replace(\"__label__\", \"\"), prediction[1][0]\n",
    "        return lang, prob\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(\n",
    "        cls,\n",
    "        *,\n",
    "        url: str = \"https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\",\n",
    "        output_dir: str = squeakily.__path__[0],\n",
    "    ):\n",
    "        path = os.path.join(output_dir, \"lid.176.bin\")\n",
    "        if not os.path.exists(path):\n",
    "            # download pretrained model with standard lib (From: https://stackoverflow.com/questions/22676/how-to-download-a-file-over-http)\n",
    "            response = urllib.request.urlretrieve(url, )\n",
    "            if response:\n",
    "                return cls(model_path=os.path.join(output_dir, \"lid.176.bin\"))\n",
    "            else:\n",
    "                raise Exception(\"Failed to download model\")\n",
    "        else:\n",
    "            return cls(model_path=path)\n",
    "    \n",
    "    def __reduce__(self):\n",
    "        return (self.__class__, (self.model_path,))\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return self.model_path == other.model_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "#|eval: false\n",
    "fasttext_model = FastTextLanguageDetector.from_pretrained()\n",
    "\n",
    "# test spanish\n",
    "lang, prob = fasttext_model.get_language(\"Hola, como estas?\")\n",
    "assert lang == \"es\"\n",
    "assert prob > 0.9\n",
    "\n",
    "# test english\n",
    "lang, prob = fasttext_model.get_language(\"Hello, how are you?\")\n",
    "assert lang == \"en\"\n",
    "assert prob > 0.9\n",
    "\n",
    "# test combination\n",
    "lang, prob = fasttext_model.get_language(\"Hello, how are you? Hola, como estas?\")\n",
    "assert prob < 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "# test with multiple lines\n",
    "\n",
    "lang, prob = fasttext_model.get_language(\"Hello, how are you?\\nI am fine, thank you.\")\n",
    "assert lang == \"en\"\n",
    "assert prob > 0.9\n",
    "\n",
    "lang, prob = fasttext_model.get_language(\"Hello, how are you?\\n\\nI am fine, thank you.\")\n",
    "assert lang == \"en\"\n",
    "assert prob > 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "#|eval: false\n",
    "# check pickling works\n",
    "import pickle\n",
    "\n",
    "with open(\"/tmp/fasttext_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(fasttext_model, f)\n",
    "\n",
    "with open(\"/tmp/fasttext_model.pkl\", \"rb\") as f:\n",
    "    pickled_fasttext_model = pickle.load(f)\n",
    "\n",
    "lang, prob = fasttext_model.get_language(\"Hello, how are you?\")\n",
    "p_lang, p_prob = pickled_fasttext_model.get_language(\"Hello, how are you?\")\n",
    "assert lang == p_lang\n",
    "assert prob == p_prob\n",
    "assert pickled_fasttext_model == fasttext_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code has been copied from the awesome Huggingface Space by [edugp](https://huggingface.co/spaces/edugp/perplexity-lenses/blob/main/perplexity_lenses/perplexity.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SentencePiece:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str,\n",
    "    ):\n",
    "        import sentencepiece\n",
    "        super().__init__()\n",
    "        self.sp = sentencepiece.SentencePieceProcessor()\n",
    "        self.sp.load(str(model))\n",
    "\n",
    "    def do(self, text: dict) -> dict:\n",
    "        tokenized = self.sp.encode_as_pieces(text)\n",
    "        return \" \".join(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "KENLM_MODEL_REPO = \"edugp/kenlm\"\n",
    "\n",
    "class KenlmModel:\n",
    "    digit_re: re.Pattern = re.compile(r\"\\d\")\n",
    "    unicode_punct: Dict[str, str] = {\n",
    "        \"，\": \",\",\n",
    "        \"。\": \".\",\n",
    "        \"、\": \",\",\n",
    "        \"„\": '\"',\n",
    "        \"”\": '\"',\n",
    "        \"“\": '\"',\n",
    "        \"«\": '\"',\n",
    "        \"»\": '\"',\n",
    "        \"１\": '\"',\n",
    "        \"」\": '\"',\n",
    "        \"「\": '\"',\n",
    "        \"《\": '\"',\n",
    "        \"》\": '\"',\n",
    "        \"´\": \"'\",\n",
    "        \"∶\": \":\",\n",
    "        \"：\": \":\",\n",
    "        \"？\": \"?\",\n",
    "        \"！\": \"!\",\n",
    "        \"（\": \"(\",\n",
    "        \"）\": \")\",\n",
    "        \"；\": \";\",\n",
    "        \"–\": \"-\",\n",
    "        \"—\": \" - \",\n",
    "        \"．\": \". \",\n",
    "        \"～\": \"~\",\n",
    "        \"’\": \"'\",\n",
    "        \"…\": \"...\",\n",
    "        \"━\": \"-\",\n",
    "        \"〈\": \"<\",\n",
    "        \"〉\": \">\",\n",
    "        \"【\": \"[\",\n",
    "        \"】\": \"]\",\n",
    "        \"％\": \"%\",\n",
    "        \"►\": \"-\",\n",
    "    }\n",
    "    unicode_punct_re = re.compile(f\"[{''.join(unicode_punct.keys())}]\")\n",
    "    non_printing_chars_re = re.compile(\n",
    "        f\"[{''.join(map(chr, list(range(0,32)) + list(range(127,160))))}]\"\n",
    "    )\n",
    "    kenlm_model_dir = None\n",
    "    sentence_piece_model_dir = None\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_dataset: str,\n",
    "        language: str,\n",
    "        lower_case: bool = False,\n",
    "        remove_accents: bool = False,\n",
    "        normalize_numbers: bool = True,\n",
    "        punctuation: int = 1,\n",
    "    ):\n",
    "        import kenlm\n",
    "\n",
    "        self.download_kenlm_model(model_dataset, language)\n",
    "        try:\n",
    "            self.model = kenlm.Model(self.kenlm_model_dir)\n",
    "            self.tokenizer = SentencePiece(self.sentence_piece_model_dir)\n",
    "        except OSError:\n",
    "            os.remove(self.kenlm_model_dir)\n",
    "            if os.path.exists(self.sentence_piece_model_dir):\n",
    "                os.remove(self.sentence_piece_model_dir)\n",
    "            raise OSError(\n",
    "                \"File was corrupt and should have been removed. Please, retry.\"\n",
    "            )\n",
    "        self.accent = remove_accents\n",
    "        self.case = lower_case\n",
    "        self.numbers = normalize_numbers\n",
    "        self.punct = punctuation\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(\n",
    "        cls,\n",
    "        *,\n",
    "        model_dataset: str,\n",
    "        language: str,\n",
    "        lower_case: bool,\n",
    "        remove_accents: bool,\n",
    "        normalize_numbers: bool,\n",
    "        punctuation: int,\n",
    "    ):\n",
    "        return cls(\n",
    "            model_dataset,\n",
    "            language,\n",
    "            lower_case,\n",
    "            remove_accents,\n",
    "            normalize_numbers,\n",
    "            punctuation,\n",
    "        )\n",
    "\n",
    "    def pp(self, log_score, length):\n",
    "        return 10.0 ** (-log_score / length)\n",
    "\n",
    "    def get_perplexity(self, doc: str, normalize_cc_net: bool = True):\n",
    "        if normalize_cc_net:\n",
    "            doc = self.normalize(\n",
    "                doc,\n",
    "                accent=self.accent,\n",
    "                case=self.case,\n",
    "                numbers=self.numbers,\n",
    "                punct=self.punct,\n",
    "            )\n",
    "        # Tokenize (after normalizing): See https://github.com/facebookresearch/cc_net/blob/bda555bd1cf1ee2e0b925363e62a61cd46c8b60d/cc_net/mine.py#L352 for full pipeline\n",
    "        doc = self.tokenizer.do(doc)\n",
    "        doc_log_score, doc_length = 0, 0\n",
    "        for line in doc.split(\"\\n\"):\n",
    "            log_score = self.model.score(line)\n",
    "            length = len(line.split()) + 1\n",
    "            doc_log_score += log_score\n",
    "            doc_length += length\n",
    "        return round(self.pp(doc_log_score, doc_length), 1)\n",
    "\n",
    "    def normalize(\n",
    "        self,\n",
    "        line: str,\n",
    "        accent: bool = True,\n",
    "        case: bool = True,\n",
    "        numbers: bool = True,\n",
    "        punct: int = 1,\n",
    "    ) -> str:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            return line\n",
    "        if case:\n",
    "            line = line.lower()\n",
    "        if accent:\n",
    "            line = self.strip_accents(line)\n",
    "        if numbers:\n",
    "            line = self.digit_re.sub(\"0\", line)\n",
    "        if punct == 1:\n",
    "            line = self.replace_unicode_punct(line)\n",
    "        elif punct == 2:\n",
    "            line = self.remove_unicode_punct(line)\n",
    "        line = self.remove_non_printing_char(line)\n",
    "        return line\n",
    "\n",
    "    def strip_accents(self, line: str) -> str:\n",
    "        \"\"\"Strips accents from a piece of text.\"\"\"\n",
    "        nfd = unicodedata.normalize(\"NFD\", line)\n",
    "        output = [c for c in nfd if unicodedata.category(c) != \"Mn\"]\n",
    "        if len(output) == line:\n",
    "            return line\n",
    "        return \"\".join(output)\n",
    "\n",
    "    def replace_unicode_punct(self, text: str) -> str:\n",
    "        return \"\".join(self.unicode_punct.get(c, c) for c in text)\n",
    "\n",
    "    def remove_unicode_punct(self, text: str) -> str:\n",
    "        \"\"\"More aggressive version of replace_unicode_punct but also faster.\"\"\"\n",
    "        return self.unicode_punct_re.sub(\"\", text)\n",
    "\n",
    "    def remove_non_printing_char(self, text: str) -> str:\n",
    "        return self.non_printing_chars_re.sub(\"\", text)\n",
    "\n",
    "    def download_kenlm_model(self, model_dataset: str, language: str):\n",
    "        try:\n",
    "            kenlm_model_url = hf_hub_url(\n",
    "                KENLM_MODEL_REPO, filename=f\"{model_dataset}/{language}.arpa.trie.bin\"\n",
    "            )\n",
    "            self.kenlm_model_dir = cached_download(kenlm_model_url)\n",
    "        except HTTPError:\n",
    "            kenlm_model_url = hf_hub_url(\n",
    "                KENLM_MODEL_REPO, filename=f\"{model_dataset}/{language}.arpa.bin\"\n",
    "            )\n",
    "            self.kenlm_model_dir = cached_download(kenlm_model_url)\n",
    "        sentence_piece_model_url = hf_hub_url(\n",
    "            KENLM_MODEL_REPO, filename=f\"{model_dataset}/{language}.sp.model\"\n",
    "        )\n",
    "        self.sentence_piece_model_dir = cached_download(sentence_piece_model_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this test, you need to have kenlm installed:\n",
    "`pip install https://github.com/kpu/kenlm/archive/master.zip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nathan/miniconda3/envs/squeakily/lib/python3.10/site-packages/huggingface_hub/file_download.py:592: FutureWarning: `cached_download` is the legacy way to download files from the HF hub, please consider upgrading to `hf_hub_download`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#|eval: false\n",
    "model = KenlmModel.from_pretrained(\n",
    "    model_dataset=\"wikipedia\",\n",
    "    language=\"en\",\n",
    "    lower_case=True,\n",
    "    remove_accents=True,\n",
    "    normalize_numbers=True,\n",
    "    punctuation=1,\n",
    ")\n",
    "\n",
    "# Get perplexity\n",
    "perplex_1 = model.get_perplexity(\"I am very perplexed\")\n",
    "perplex_2 = model.get_perplexity(\"im hella trippin\")\n",
    "\n",
    "assert perplex_1 < perplex_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('squeakily')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
