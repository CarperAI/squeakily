{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clean\n",
    "\n",
    "> This module contains all the various cleaning options supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import re\n",
    "from faker import Faker\n",
    "import ftfy\n",
    "\n",
    "fake = Faker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "# From: https://github.com/bigscience-workshop/data-preparation/blob/main/preprocessing/training/01b_oscar_cleaning_and_filtering/filtering.py#L95\n",
    "whitespace = {\n",
    "    \" \",\n",
    "    \" \",\n",
    "    \" \",\n",
    "    \" \",\n",
    "    \" \",\n",
    "    \"　\",\n",
    "    \" \",\n",
    "    \" \",\n",
    "    \" \",\n",
    "    \" \",\n",
    "    \"￼\",\n",
    "    \"\",\n",
    "}\n",
    "\n",
    "\n",
    "def normalize_whitespace(\n",
    "    text: str,  # The text to normalize\n",
    ") -> str:  # The normalized text\n",
    "    \"\"\"\n",
    "    Replace the various whitespace characters with the standard one.\n",
    "    \"\"\"\n",
    "    text = \"\".join([char if char not in whitespace else \" \" for char in text])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the normalize_whitespace function\n",
    "assert normalize_whitespace(\"a b c d e　f g h i￼jk\") == \"a b c d e f g h i j k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "unicode_punctuation = {\n",
    "    \"，\": \",\",\n",
    "    \"。\": \".\",\n",
    "    \"、\": \",\",\n",
    "    \"„\": '\"',\n",
    "    \"”\": '\"',\n",
    "    \"“\": '\"',\n",
    "    \"«\": '\"',\n",
    "    \"»\": '\"',\n",
    "    \"１\": '\"',\n",
    "    \"」\": '\"',\n",
    "    \"「\": '\"',\n",
    "    \"《\": '\"',\n",
    "    \"》\": '\"',\n",
    "    \"´\": \"'\",\n",
    "    \"∶\": \":\",\n",
    "    \"：\": \":\",\n",
    "    \"？\": \"?\",\n",
    "    \"！\": \"!\",\n",
    "    \"（\": \"(\",\n",
    "    \"）\": \")\",\n",
    "    \"；\": \";\",\n",
    "    \"–\": \"-\",\n",
    "    \"—\": \" - \",\n",
    "    \"．\": \". \",\n",
    "    \"～\": \"~\",\n",
    "    \"’\": \"'\",\n",
    "    \"…\": \"...\",\n",
    "    \"━\": \"-\",\n",
    "    \"〈\": \"<\",\n",
    "    \"〉\": \">\",\n",
    "    \"【\": \"[\",\n",
    "    \"】\": \"]\",\n",
    "    \"％\": \"%\",\n",
    "    \"►\": \"-\",\n",
    "}\n",
    "\n",
    "\n",
    "def normalize_punctuation(\n",
    "    text: str,  # The text to normalize\n",
    ") -> str:  # The normalized text\n",
    "    \"\"\"\n",
    "    Replace the various unicode punctuation characters with the standard ones.\n",
    "    \"\"\"\n",
    "    text = \"\".join([unicode_punctuation.get(char, char) for char in text])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the normalize_punctuation function\n",
    "text = \"，。、„”“«»１」「《》´∶：？！（）；–—．～’…━〈〉【】％►\"\n",
    "\n",
    "assert normalize_punctuation(text) == ',.,\"\"\"\"\"\"\"\"\"\"\\'::?!();- - . ~\\'...-<>[]%-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def remove_empty_lines(\n",
    "    text: str,  # The text to remove empty lines from\n",
    ") -> str:  # The text with empty lines removed\n",
    "    \"\"\"\n",
    "    Remove empty lines from the text.\n",
    "    Solution from https://stackoverflow.com/a/3711884/5768407\n",
    "    \"\"\"\n",
    "    lines = text.splitlines()\n",
    "    filtered = filter(lambda x: not re.match(r\"^\\s*$\", x), lines)\n",
    "    return \"\\n\".join(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the remove_empty_lines function\n",
    "starts_with_newline = \"\\nfoo\\nbar\"\n",
    "multiple_newlines = \"foo\\n\\nbar\"\n",
    "ends_with_newline = \"foo\\nbar\\n\"\n",
    "\n",
    "assert remove_empty_lines(starts_with_newline) == \"foo\\nbar\"\n",
    "assert remove_empty_lines(multiple_newlines) == \"foo\\nbar\"\n",
    "assert remove_empty_lines(ends_with_newline) == \"foo\\nbar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def replace_urls(\n",
    "    text: str,  # The text to replace URLs in\n",
    "    dummy: str = \"https://example.com/\",  # The dummy text to replace URLs with\n",
    ") -> str:  # The text with URLs replaced\n",
    "    \"\"\"Replace urls from text with a dummy.\"\"\"\n",
    "    return re.sub(r\"http\\S+\", dummy, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the replace_urls function\n",
    "url_after_space = \"foo http://bar.com\"\n",
    "url_before_space = \"http://foo.com bar\"\n",
    "assert replace_urls(url_after_space) == \"foo https://example.com/\"\n",
    "assert replace_urls(url_before_space) == \"https://example.com/ bar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def replace_dates(\n",
    "    text: str,  # The text to remove dates from\n",
    "    dummy: str = fake.date(),  # The dummy text to replace dates with\n",
    ") -> str:  # The text with dates replaced\n",
    "    \"\"\"Replace dates from text with a dummy.\"\"\"\n",
    "    return re.sub(r\"\\d{1,2}/\\d{1,2}/\\d{4}\", dummy, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the replace_dates function\n",
    "date_after_space = \"foo 1/1/2020\"\n",
    "date_before_space = \"1/1/2020 bar\"\n",
    "assert replace_dates(date_after_space, \"1/1/1970\") == \"foo 1/1/1970\"\n",
    "assert replace_dates(date_before_space, \"1/1/1970\") == \"1/1/1970 bar\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PII Removal\n",
    "\n",
    "Currently, we support the following PII removal options:\n",
    "\n",
    "  * `replace_email`\n",
    "  * `replace_phone`\n",
    "  * `replace_ip`\n",
    "  * `replace_credit_card`\n",
    "  * `replace_ssn`\n",
    "\n",
    "However, for emails, phone numbers, credit cards, and SSNs, we recommend you to use the [scrubadub](https://scrubadub.readthedocs.io/en/stable/index.html) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def replace_email(\n",
    "    text: str,  # The text to replace email addresses in\n",
    "    dummy: str = fake.email(),  # The dummy text to replace email addresses with\n",
    ") -> str:  # The text with email addresses replaced\n",
    "    \"\"\"Replace email addresses from text with a dummy.\"\"\"\n",
    "    return re.sub(r\"[\\w\\.-]+@[\\w\\.-]+\", dummy, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the replace_email function\n",
    "email_after_space = \"foo fake@email.com\"\n",
    "email_before_space = \"fake@email.com bar\"\n",
    "email_with_forward_periods = \"foo.bar@email.com\"\n",
    "email_with_backward_periods = \"foo@bar.email.com\"\n",
    "\n",
    "assert replace_email(email_after_space, \"example@email.com\") == \"foo example@email.com\"\n",
    "assert replace_email(email_before_space, \"example@email.com\") == \"example@email.com bar\"\n",
    "assert (\n",
    "    replace_email(email_with_forward_periods, \"example@email.com\")\n",
    "    == \"example@email.com\"\n",
    ")\n",
    "assert (\n",
    "    replace_email(email_with_backward_periods, \"example@email.com\")\n",
    "    == \"example@email.com\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def replace_phone(\n",
    "    text: str,  # The text to replace phone numbers in\n",
    "    dummy: str = fake.phone_number(),  # The dummy text to replace phone numbers with\n",
    ") -> str:  # The text with phone numbers replaced\n",
    "    \"\"\"Replace phone numbers from text with a dummy.\"\"\"\n",
    "    return re.sub(r\"\\(?\\d{3}\\)?-? *\\d{3}-? *-?\\d{4}\", dummy, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the replace_phone function\n",
    "phone_after_space = \"foo 111-222-3333\"\n",
    "phone_before_space = \"111-222-3333 bar\"\n",
    "phone_with_parens = \"(111) 222-3333\"\n",
    "phone_with_spaces = \"111 222 3333\"\n",
    "phone_with_dashes = \"111-222-3333\"\n",
    "\n",
    "assert replace_phone(phone_after_space, \"123-456-7890\") == \"foo 123-456-7890\"\n",
    "assert replace_phone(phone_before_space, \"123-456-7890\") == \"123-456-7890 bar\"\n",
    "assert replace_phone(phone_with_parens, \"123-456-7890\") == \"123-456-7890\"\n",
    "assert replace_phone(phone_with_spaces, \"123-456-7890\") == \"123-456-7890\"\n",
    "assert replace_phone(phone_with_dashes, \"123-456-7890\") == \"123-456-7890\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def replace_ip(\n",
    "    text,  # The text to replace ip addresses in\n",
    "    dummy1: str = fake.ipv4(),  # The dummy text to replace ipv4 addresses with\n",
    "    dummy2: str = fake.ipv6(),  # The dummy text to replace ipv6 addresses with\n",
    ") -> str:  # The text with ip addresses replaced\n",
    "    \"\"\"\n",
    "    Replace ip addresses from text with a dummy.\n",
    "    Solution from https://github.com/bigcode-project/bigcode-analysis/blob/main/data_analysis/pii/utils/emails_ip_addresses_detection.py#L48\n",
    "    \"\"\"\n",
    "    ipv4_pattern = r\"(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(?:\\.(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}\"\n",
    "    text = re.sub(ipv4_pattern, dummy1, text)\n",
    "    ipv6_pattern = r\"(?:[0-9a-fA-F]{1,4}:){7,7}[0-9a-fA-F]{1,4}|(?:[0-9a-fA-F]{1,4}:){1,7}:|(?:[0-9a-fA-F]{1,4}:){1,6}:[0-9a-fA-F]{1,4}|(?:[0-9a-fA-F]{1,4}:){1,5}(?::[0-9a-fA-F]{1,4}){1,2}|(?:[0-9a-fA-F]{1,4}:){1,4}(?::[0-9a-fA-F]{1,4}){1,3}|(?:[0-9a-fA-F]{1,4}:){1,3}(?::[0-9a-fA-F]{1,4}){1,4}|(?:[0-9a-fA-F]{1,4}:){1,2}(?::[0-9a-fA-F]{1,4}){1,5}|[0-9a-fA-F]{1,4}:(?:(?::[0-9a-fA-F]{1,4}){1,6})|:(?:(?::[0-9a-fA-F]{1,4}){1,7}|:)|fe80:(?::[0-9a-fA-F]{0,4}){0,4}%[0-9a-zA-Z]{1,}|::(?:ffff(?::0{1,4}){0,1}:){0,1}(?:(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])|(?:[0-9a-fA-F]{1,4}:){1,4}:(?:(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])\"\n",
    "    text = re.sub(ipv6_pattern, dummy2, text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the replace_ip function\n",
    "ip4_after_space = \"foo 111.222.3.4\"\n",
    "ip4_before_space = \"111.222.3.4 bar\"\n",
    "ip6_with_colons = \"2001:0db8:0000:0000:0000:8a2e:0370:7334\"\n",
    "\n",
    "assert replace_ip(ip4_after_space, \"127.0.0.1\") == \"foo 127.0.0.1\"\n",
    "assert replace_ip(ip4_before_space, \"127.0.0.1\") == \"127.0.0.1 bar\"\n",
    "assert replace_ip(ip6_with_colons, \"127.0.0.1\", \"0:0:0:0:0:0:0:1\") == \"0:0:0:0:0:0:0:1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def replace_credit_card(\n",
    "    text: str,  # The text to replace credit card numbers in\n",
    "    dummy: str = fake.credit_card_number(),  # The dummy text to replace credit card numbers with\n",
    ") -> str:  # The text with credit card numbers replaced\n",
    "    \"\"\"Replace credit card numbers from text with a dummy.\"\"\"\n",
    "    return re.sub(r\"\\d{4}-\\d{4}-\\d{4}-\\d{4}\", dummy, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the replace_credit_card function\n",
    "credit_card_after_space = \"foo 1111-2222-3333-4444\"\n",
    "credit_card_before_space = \"1111-2222-3333-4444 bar\"\n",
    "\n",
    "assert (\n",
    "    replace_credit_card(credit_card_after_space, \"1234-5678-9012-3456\")\n",
    "    == \"foo 1234-5678-9012-3456\"\n",
    ")\n",
    "assert (\n",
    "    replace_credit_card(credit_card_before_space, \"1234-5678-9012-3456\")\n",
    "    == \"1234-5678-9012-3456 bar\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def replace_ssn(\n",
    "    text: str,  # The text to replace social security numbers in\n",
    "    dummy: str = fake.ssn(),  # The dummy text to replace social security numbers with\n",
    ") -> str:  # The text with social security numbers replaced\n",
    "    \"\"\"Replace social security numbers from text with a dummy.\"\"\"\n",
    "    return re.sub(r\"\\d{3}-\\d{2}-\\d{4}\", dummy, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the replace_ssn function\n",
    "ssn_after_space = \"foo 111-22-3333\"\n",
    "ssn_before_space = \"111-22-3333 bar\"\n",
    "\n",
    "assert replace_ssn(ssn_after_space, \"123-45-6789\") == \"foo 123-45-6789\"\n",
    "assert replace_ssn(ssn_before_space, \"123-45-6789\") == \"123-45-6789 bar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def fix_utf8_encoding(\n",
    "    text: str,  # The text to fix\n",
    ") -> str:  # The fixed text\n",
    "    \"\"\"Fix utf8 text using ftfy.\"\"\"\n",
    "    return ftfy.fix_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the fix_utf8_encoding function\n",
    "bad_text = \"âœ” No problems\"\n",
    "assert fix_utf8_encoding(bad_text) == \"✔ No problems\"\n",
    "bad_text = \"dÃ©jÃ  vu\"\n",
    "assert fix_utf8_encoding(bad_text) == \"déjà vu\"\n",
    "bad_text = \"Ã©\"\n",
    "assert fix_utf8_encoding(bad_text) == \"é\"\n",
    "bad_text = \"P&EACUTE;REZ\"\n",
    "assert fix_utf8_encoding(bad_text) == \"PÉREZ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def clean_code_license(\n",
    "    code: str,  # The code to clean\n",
    "    language: str = \"python\",  # The language of the code\n",
    "    min_lines: int = 3,  # The minimum number of lines that need to be removed\n",
    "):\n",
    "    import code_ast\n",
    "    from code_ast import ASTVisitor\n",
    "    from code_ast.ast import LEAVE_WHITELIST\n",
    "\n",
    "    class FirstNonCommentVisitor(ASTVisitor):\n",
    "        def __init__(self):\n",
    "            self.passed_global_node = False\n",
    "            self.first_node = None\n",
    "\n",
    "        def visit(self, node):\n",
    "            if not self.passed_global_node:\n",
    "                self.passed_global_node = True\n",
    "                return\n",
    "            if self.first_node is None:\n",
    "                if node.child_count > 0 or node.type in LEAVE_WHITELIST:\n",
    "                    self.first_node = node\n",
    "\n",
    "    \"\"\"Remove the license or other boilerplate comments from the code.\"\"\"\n",
    "    ast = code_ast.ast(code, lang=language)\n",
    "    visitor = FirstNonCommentVisitor()\n",
    "    ast.visit(visitor)\n",
    "    start_line = visitor.first_node.start_point[0]\n",
    "    if start_line < min_lines:\n",
    "        return code\n",
    "    else:\n",
    "        return \"\\n\".join(code.splitlines()[start_line:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |eval: false\n",
    "# Test the cleaning of code licenses or similar boilerplate comments from code\n",
    "code_python = \"\"\"# -*- coding: utf-8 -*-\n",
    "\n",
    "# Copyright 2018 Spanish National Research Council (CSIC)\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n",
    "# not use this file except in compliance with the License. You may obtain\n",
    "# a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n",
    "# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n",
    "# License for the specific language governing permissions and limitations\n",
    "# under the License.\n",
    "\n",
    "\\\"\\\"\\\"\n",
    "Given two dates and region, download N Sentinel Collections scenes from ESA\n",
    "Sentinel dataHUB.\n",
    "The downloaded Sentinel collection scenes are compatible with:\n",
    "S2MSI1C: Top-of-atmosphere reflectances in cartographic geometry\n",
    "or S2MSI2A: Bottom-of-atmosphere reflectance in cartographic geometry\n",
    "Parameters\n",
    "----------\n",
    "inidate: datetime.strptime(\"YYYY-MM-dd\", \"%Y-%m-%d\")\n",
    "enddate: datetime.strptime(\"YYYY-MM-dd\", \"%Y-%m-%d\")\n",
    "region: name of one reservoir saved in the \"coord_reservoirs.json\" file\n",
    "coordinates : dict. Coordinates of the region to search.\n",
    "Example: {\"W\": -2.830, \"S\": 41.820, \"E\": -2.690, \"N\": 41.910}}\n",
    "platform : str. Satellite to use from the Sentinel family\n",
    "producttype : str. Dataset type.\n",
    "cloud: int\n",
    "path : path\n",
    "Author: Daniel García Díaz\n",
    "Email: garciad@ifca.unican.es\n",
    "Institute of Physics of Cantabria (IFCA)\n",
    "Advanced Computing and e-Science\n",
    "Date: Sep 2018\n",
    "\\\"\\\"\\\"\n",
    "#imports apis\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# Subfunctions\n",
    "from wq_sat.utils import config\n",
    "\"\"\"\n",
    "\n",
    "code_go = \"\"\"// +build go1.9\n",
    "\n",
    "// Copyright 2019 Microsoft Corporation\n",
    "//\n",
    "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "// you may not use this file except in compliance with the License.\n",
    "// You may obtain a copy of the License at\n",
    "//\n",
    "//     http://www.apache.org/licenses/LICENSE-2.0\n",
    "//\n",
    "// Unless required by applicable law or agreed to in writing, software\n",
    "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "// See the License for the specific language governing permissions and\n",
    "// limitations under the License.\n",
    "\n",
    "// This code was auto-generated by:\n",
    "// github.com/Azure/azure-sdk-for-go/tools/profileBuilder\n",
    "\n",
    "package policyinsights\n",
    "\n",
    "import (\n",
    "\t\"context\"\n",
    "\n",
    "\toriginal \"github.com/Azure/azure-sdk-for-go/services/policyinsights/mgmt/2019-10-01/policyinsights\"\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "code_c = \"\"\"/*\n",
    " * copyright (c) 2008 - 2011 Espressif System\n",
    " *\n",
    " * Define user specified Event signals and Task priorities here\n",
    " *\n",
    " */\n",
    "\n",
    "#ifndef _ETS_SYS_H\n",
    "#define _ETS_SYS_H\n",
    "\n",
    "#include \"c_types.h\"\n",
    "#include \"eagle_soc.h\"\n",
    "\n",
    "typedef uint32_t ETSSignal;\n",
    "\"\"\"\n",
    "\n",
    "code_cpp = \"\"\"/*  Pokemon Automation Bot Base - Client Example\n",
    "\n",
    " * \n",
    "\n",
    " *  From: https://github.com/PokemonAutomation/Arduino-Source\n",
    "\n",
    " * \n",
    "\n",
    " */\n",
    "\n",
    "\n",
    "\n",
    "#include \"Common/CRC32.h\"\n",
    "\n",
    "#include \"Common/Microcontroller/MessageProtocol.h\"\n",
    "\n",
    "#include \"ClientSource/Libraries/Logging.h\"\n",
    "\n",
    "#include \"ClientSource/Libraries/MessageConverter.h\"\n",
    "\n",
    "#include \"BotBaseMessage.h\"\n",
    "\n",
    "#include \"PABotBaseConnection.h\"\n",
    "\n",
    "\n",
    "\n",
    "#include <iostream>\n",
    "\n",
    "using std::cout;\n",
    "\"\"\"\n",
    "\n",
    "code_java = \"\"\"/*\n",
    " * Copyright (C) 2012-2021 DuyHai DOAN\n",
    " *\n",
    " * Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    " * you may not use this file except in compliance with the License.\n",
    " * You may obtain a copy of the License at\n",
    " *\n",
    " * http://www.apache.org/licenses/LICENSE-2.0\n",
    " *\n",
    " * Unless required by applicable law or agreed to in writing, software\n",
    " * distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    " * See the License for the specific language governing permissions and\n",
    " * limitations under the License.\n",
    " */\n",
    "\n",
    "package info.archinnov.achilles.internals.sample_classes.parser.entity;\n",
    "\n",
    "import info.archinnov.achilles.annotations.Column;\n",
    "\"\"\"\n",
    "\n",
    "code_javascript = \"\"\"/*\n",
    "** Copyright (c) 2016-2019, Thomas Farr\n",
    "**\n",
    "** This Source Code Form is subject to the terms of the Mozilla Public\n",
    "** License, v. 2.0. If a copy of the MPL was not distributed with this\n",
    "** file, You can obtain one at https://mozilla.org/MPL/2.0/.\n",
    "*/\n",
    "\n",
    "// TODO: Implement testing of option handling, and filename arrays\n",
    "\n",
    "const anitomy = require('../anitomy');\n",
    "const async = require('async');\n",
    "\"\"\"\n",
    "\n",
    "cleaned_code_python = clean_code_license(code_python, language=\"python\")\n",
    "cleaned_code_go = clean_code_license(code_go, language=\"go\")\n",
    "cleaned_code_c = clean_code_license(code_c, language=\"c\")\n",
    "cleaned_code_cpp = clean_code_license(code_cpp, language=\"cpp\")\n",
    "cleaned_code_java = clean_code_license(code_java, language=\"java\")\n",
    "cleaned_code_javascript = clean_code_license(code_javascript, language=\"javascript\")\n",
    "\n",
    "assert (\n",
    "    cleaned_code_python\n",
    "    == \"\"\"\\\"\\\"\\\"\n",
    "Given two dates and region, download N Sentinel Collections scenes from ESA\n",
    "Sentinel dataHUB.\n",
    "The downloaded Sentinel collection scenes are compatible with:\n",
    "S2MSI1C: Top-of-atmosphere reflectances in cartographic geometry\n",
    "or S2MSI2A: Bottom-of-atmosphere reflectance in cartographic geometry\n",
    "Parameters\n",
    "----------\n",
    "inidate: datetime.strptime(\"YYYY-MM-dd\", \"%Y-%m-%d\")\n",
    "enddate: datetime.strptime(\"YYYY-MM-dd\", \"%Y-%m-%d\")\n",
    "region: name of one reservoir saved in the \"coord_reservoirs.json\" file\n",
    "coordinates : dict. Coordinates of the region to search.\n",
    "Example: {\"W\": -2.830, \"S\": 41.820, \"E\": -2.690, \"N\": 41.910}}\n",
    "platform : str. Satellite to use from the Sentinel family\n",
    "producttype : str. Dataset type.\n",
    "cloud: int\n",
    "path : path\n",
    "Author: Daniel García Díaz\n",
    "Email: garciad@ifca.unican.es\n",
    "Institute of Physics of Cantabria (IFCA)\n",
    "Advanced Computing and e-Science\n",
    "Date: Sep 2018\n",
    "\\\"\\\"\\\"\n",
    "#imports apis\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# Subfunctions\n",
    "from wq_sat.utils import config\"\"\"\n",
    ")\n",
    "assert (\n",
    "    cleaned_code_go\n",
    "    == \"\"\"package policyinsights\n",
    "\n",
    "import (\n",
    "\t\"context\"\n",
    "\n",
    "\toriginal \"github.com/Azure/azure-sdk-for-go/services/policyinsights/mgmt/2019-10-01/policyinsights\"\n",
    ")\"\"\"\n",
    ")\n",
    "assert (\n",
    "    cleaned_code_c\n",
    "    == \"\"\"#ifndef _ETS_SYS_H\n",
    "#define _ETS_SYS_H\n",
    "\n",
    "#include \"c_types.h\"\n",
    "#include \"eagle_soc.h\"\n",
    "\n",
    "typedef uint32_t ETSSignal;\"\"\"\n",
    ")\n",
    "assert (\n",
    "    cleaned_code_cpp\n",
    "    == \"\"\"#include \"Common/CRC32.h\"\n",
    "\n",
    "#include \"Common/Microcontroller/MessageProtocol.h\"\n",
    "\n",
    "#include \"ClientSource/Libraries/Logging.h\"\n",
    "\n",
    "#include \"ClientSource/Libraries/MessageConverter.h\"\n",
    "\n",
    "#include \"BotBaseMessage.h\"\n",
    "\n",
    "#include \"PABotBaseConnection.h\"\n",
    "\n",
    "\n",
    "\n",
    "#include <iostream>\n",
    "\n",
    "using std::cout;\"\"\"\n",
    ")\n",
    "assert (\n",
    "    cleaned_code_java\n",
    "    == \"\"\"package info.archinnov.achilles.internals.sample_classes.parser.entity;\n",
    "\n",
    "import info.archinnov.achilles.annotations.Column;\"\"\"\n",
    ")\n",
    "assert (\n",
    "    cleaned_code_javascript\n",
    "    == \"\"\"const anitomy = require('../anitomy');\n",
    "const async = require('async');\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
