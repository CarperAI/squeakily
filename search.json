[
  {
    "objectID": "helpers.html",
    "href": "helpers.html",
    "title": "helpers",
    "section": "",
    "text": "Important\n\n\n\nNote: This notebook contains a large collection of profane and offensive language to use as a word filter. It is not recommended for children or the highly sensitive.\n\n\n\nsource\n\nget_words\n\n get_words (text:str)\n\ncustom regex to extract all the words in a string\n\n\n\n\nType\nDetails\n\n\n\n\ntext\nstr\nthe text to extract words from\n\n\nReturns\nlist\n\n\n\n\nThe following code is adapted from this awesome blog post by C Chaitanya.\n\nsource\n\n\nFastTextLanguageDetector\n\n FastTextLanguageDetector (model_path:str='/tmp/lid.176.bin')\n\nInitialize self. See help(type(self)) for accurate signature.\n\nfasttext_model = FastTextLanguageDetector.from_pretrained()\n\n# test spanish\nlang, prob = fasttext_model.get_language(\"Hola, como estas?\")\nassert lang == \"es\"\nassert prob &gt; 0.9\n\n# test english\nlang, prob = fasttext_model.get_language(\"Hello, how are you?\")\nassert lang == \"en\"\nassert prob &gt; 0.9\n\n# test combination\nlang, prob = fasttext_model.get_language(\"Hello, how are you? Hola, como estas?\")\nassert prob &lt; 0.9\n\nWarning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n\n\n\n# test with multiple lines\n\nlang, prob = fasttext_model.get_language(\"Hello, how are you?\\nI am fine, thank you.\")\nassert lang == \"en\"\nassert prob &gt; 0.9\n\nlang, prob = fasttext_model.get_language(\"Hello, how are you?\\n\\nI am fine, thank you.\")\nassert lang == \"en\"\nassert prob &gt; 0.9\n\n\n# check pickling works\nimport pickle\n\nwith open(\"/tmp/fasttext_model.pkl\", \"wb\") as f:\n    pickle.dump(fasttext_model, f)\n\nwith open(\"/tmp/fasttext_model.pkl\", \"rb\") as f:\n    pickled_fasttext_model = pickle.load(f)\n\nlang, prob = fasttext_model.get_language(\"Hello, how are you?\")\np_lang, p_prob = pickled_fasttext_model.get_language(\"Hello, how are you?\")\nassert lang == p_lang\nassert prob == p_prob\nassert pickled_fasttext_model == fasttext_model\n\nWarning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n\n\nThe following code has been copied from the awesome Huggingface Space by edugp\n\nsource\n\n\nSentencePiece\n\n SentencePiece (model:str)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nKenlmModel\n\n KenlmModel (model_dataset:str, language:str, lower_case:bool=False,\n             remove_accents:bool=False, normalize_numbers:bool=True,\n             punctuation:int=1)\n\nInitialize self. See help(type(self)) for accurate signature.\nTo run this test, you need to have kenlm installed: pip install https://github.com/kpu/kenlm/archive/master.zip\n\nmodel = KenlmModel.from_pretrained(\n    model_dataset=\"wikipedia\",\n    language=\"en\",\n    lower_case=True,\n    remove_accents=True,\n    normalize_numbers=True,\n    punctuation=1,\n)\n\n# Get perplexity\nperplex_1 = model.get_perplexity(\"I am very perplexed\")\nperplex_2 = model.get_perplexity(\"im hella trippin\")\n\nassert perplex_1 &lt; perplex_2\n\n/home/nathan/miniconda3/envs/squeakily/lib/python3.10/site-packages/huggingface_hub/file_download.py:592: FutureWarning: `cached_download` is the legacy way to download files from the HF hub, please consider upgrading to `hf_hub_download`\n  warnings.warn("
  },
  {
    "objectID": "filter.html",
    "href": "filter.html",
    "title": "filter",
    "section": "",
    "text": "source"
  },
  {
    "objectID": "filter.html#whole-dataset-filtering",
    "href": "filter.html#whole-dataset-filtering",
    "title": "filter",
    "section": "Whole Dataset Filtering",
    "text": "Whole Dataset Filtering\n\nMinHash Deduplication\nThe following code has all been adapted from the awesome Chenghao Mou and their work on the BigCode repository!\n\nresult = _hash_func(0, \"Hello world!\", num_perm=128)\nassert result[\"__id__\"] == 0\nassert result[\"__signature__\"].shape == (128,)\nassert result[\"__signature__\"].dtype == np.uint64\n\n\ndata = [\"Hello world!\", \"Hello world\"]\nsignatures = [_hash_func(i, content, num_perm=128) for i, content in enumerate(data)]\nindex = MinHashLSH(threshold=0.5, num_perm=128)\nfor signature in signatures:\n    index.insert(\n        signature[\"__id__\"],\n        MinHash(num_perm=128, hashvalues=signature[\"__signature__\"], seed=MINHASH_SEED)\n    )\nassert _query_content(0, signatures[0][\"__signature__\"], index=index) == {'__neighbors__': [1], '__id__': 0}\nassert _query_content(1, signatures[1][\"__signature__\"], index=index) == {'__neighbors__': [0], '__id__': 1}\n\n\nassert _jaccard_similarity(\"a = 1\", \"a = 2\") == 0.3333333333333333\nassert _jaccard_similarity(\"a = 1\", \"a = 1\") == 1.0\n\n\nsource\n\n\nminhash_dedup\n\n minhash_dedup (ds, column, community_detection:bool=False,\n                report_false_positive_rate:bool=False,\n                threshold:float=0.85, num_perm:int=128,\n                dry_run:bool=False)\n\nDeduplicate the dataset using minhashing as described in the paper “Deduplicating Training Data Makes Language Models Better”.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nds\n\n\nThe dataset to deduplicate.\n\n\ncolumn\n\n\nThe column to use for deduplication.\n\n\ncommunity_detection\nbool\nFalse\nWhether to use community detection to find the duplicate communities, or to use the connected components.\n\n\nreport_false_positive_rate\nbool\nFalse\nWhether to report the false positive rate.\n\n\nthreshold\nfloat\n0.85\nThe threshold to use for deduplication.\n\n\nnum_perm\nint\n128\nThe number of permutations to use for minhashing.\n\n\ndry_run\nbool\nFalse\nWhether to run the deduplication in dry run mode.\n\n\nReturns\nDataset\n\n\n\n\n\n\ndataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"train\")\ndataset = dataset.select(range(1_000))\ndeduped_dataset = minhash_dedup(dataset, \"text\", community_detection=True, threshold=0.85, num_perm=128)\n\nassert len(deduped_dataset) == len(dataset) - len(dup_ids)\nassert deduped_dataset.column_names == dataset.column_names + [\"__id__\"]\n\n                                                                                                                                                                \n\n\n\n\n\n\n\n\n\n\n\n\n# test dry run\ndeduped_dataset = minhash_dedup(dataset, \"text\", community_detection=True, threshold=0.85, num_perm=128, dry_run=True)\n\nassert len(deduped_dataset) == len(dataset)\nassert deduped_dataset.column_names == dataset.column_names + [\"__id__\", \"duplicate\"]\n\n                                                                                                                                                                \n\n\n\n\n\n\n\n\n\n\n\n\n# print which records were removed\nfor idx in dup_ids:\n    if dataset[idx][\"text\"] == \"\":\n        continue\n    print(dataset[idx][\"text\"])\n\n Flower Fairies of the Spring ; Blackie , 1923 \n\n = = = Regular season = = = \n\n \" There 's Got to Be a Way \" ( 12 \" remix ) \n\n = = Early life = = \n\n = = Awards = = \n\n = = Critical reception = = \n\n = = History = = \n\n = = Service history = = \n\n = = Description = = \n\n = = Background = ="
  },
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "Tutorial: Using another library",
    "section": "",
    "text": "First off, we need to install the library.\npip install scrubadub\nNow we will use the same (wikitext) dataset as in the previous tutorial.\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"wikitext\", \"wikitext-103-v1\", split=\"train[:1%]\")\n\nWe will use the scrubadub library to remove personal information from the text. scrubadub usually defaults to removing the following types: * credential - username and password combinations * credit_card - credit card numbers * drivers_license - drivers license numbers * email - email addresses * national_insurance_number - GB National Insurance numbers (NINOs) * phone - phone numbers * postalcode - british postal codes * social_security_number - US Social Security numbers (SSNs) * tax_reference_number - UK PAYE temporary reference number (TRN) * twitter - twitter handles * url - URLs * vehicle_license_plate - british vehicle license plates\nHowever, while experimenting with the library it seems some of these are not on by default. Either way, we are only going to focus on the credit_card, drivers_license, email, phone, and social_security_number detectors. Therefore, we must turn the others off:\n\nfrom scrubadub import Scrubber\nfrom scrubadub.detectors import CredentialDetector, TwitterDetector, UrlDetector\n\nscrubber = Scrubber()\nscrubber.remove_detector(CredentialDetector)\nscrubber.remove_detector(TwitterDetector)\nscrubber.remove_detector(UrlDetector)\n\ndatasources = [\n    {\n        \"dataset\": ds,\n        \"name\": \"wikitext\",\n        \"columns\": [\"text\"],\n        \"filters\": [],\n        \"cleaners\": [scrubber.clean],\n    },\n    # ...\n]\n\nEssentially, any function that takes in a string and returns a string will work out of the box with squeakily. Luckily for us, scrubadub has a clean function that does just that. We can use this function to remove personal information from the text!\nA similar process can be used for filters, except the return type is a bool instead of a str denoting whether or not the text should be kept.\n\n\n\n\n\n\nNote\n\n\n\nNote: If you want to mix and match, it is super easy!\nfrom squeakily.clean import remove_empty_lines, remove_ip\ndatasources = [\n    {\n        \"dataset\": ds,\n        \"name\": \"wikitext\",\n        \"columns\": [\"text\"],\n        \"filters\": [],\n        \"cleaners\": [scrubber.clean, remove_empty_lines, remove_ip],\n    },\n    # ...\n]\n\n\nNow we can process the datasources as before with a Pipeline object.\n\nfrom squeakily.core import Pipeline\n\npipeline = Pipeline(datasources)\npipeline.run()"
  },
  {
    "objectID": "clean.html",
    "href": "clean.html",
    "title": "clean",
    "section": "",
    "text": "source"
  },
  {
    "objectID": "clean.html#pii-removal",
    "href": "clean.html#pii-removal",
    "title": "clean",
    "section": "PII Removal",
    "text": "PII Removal\nCurrently, we support the following PII removal options:\n\nreplace_email\nreplace_phone\nreplace_ip\nreplace_credit_card\nreplace_ssn\n\nHowever, for emails, phone numbers, credit cards, and SSNs, we recommend you to use the scrubadub library.\n\nsource\n\nreplace_email\n\n replace_email (text:str, dummy:str='gaustin@example.org')\n\nReplace email addresses from text with a dummy.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntext\nstr\n\nThe text to replace email addresses in\n\n\ndummy\nstr\ngaustin@example.org\nThe dummy text to replace email addresses with\n\n\nReturns\nstr\n\nThe text with email addresses replaced\n\n\n\n\n# test the replace_email function\nemail_after_space = \"foo fake@email.com\"\nemail_before_space = \"fake@email.com bar\"\nemail_with_forward_periods = \"foo.bar@email.com\"\nemail_with_backward_periods = \"foo@bar.email.com\"\n\nassert replace_email(email_after_space, \"example@email.com\") == \"foo example@email.com\"\nassert replace_email(email_before_space, \"example@email.com\") == \"example@email.com bar\"\nassert replace_email(email_with_forward_periods, \"example@email.com\") == \"example@email.com\"\nassert replace_email(email_with_backward_periods, \"example@email.com\") == \"example@email.com\"\n\n\nsource\n\n\nreplace_phone\n\n replace_phone (text:str, dummy:str='267.517.3897')\n\nReplace phone numbers from text with a dummy.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntext\nstr\n\nThe text to replace phone numbers in\n\n\ndummy\nstr\n267.517.3897\nThe dummy text to replace phone numbers with\n\n\nReturns\nstr\n\nThe text with phone numbers replaced\n\n\n\n\n# test the replace_phone function\nphone_after_space = \"foo 111-222-3333\"\nphone_before_space = \"111-222-3333 bar\"\nphone_with_parens = \"(111) 222-3333\"\nphone_with_spaces = \"111 222 3333\"\nphone_with_dashes = \"111-222-3333\"\n\nassert replace_phone(phone_after_space, \"123-456-7890\") == \"foo 123-456-7890\"\nassert replace_phone(phone_before_space, \"123-456-7890\") == \"123-456-7890 bar\"\nassert replace_phone(phone_with_parens, \"123-456-7890\") == \"123-456-7890\"\nassert replace_phone(phone_with_spaces, \"123-456-7890\") == \"123-456-7890\"\nassert replace_phone(phone_with_dashes, \"123-456-7890\") == \"123-456-7890\"\n\n\nsource\n\n\nreplace_ip\n\n replace_ip (text, dummy1:str='84.107.244.23',\n             dummy2:str='db2c:7ab5:1955:85ff:dfcd:786a:b58b:afe0')\n\nReplace ip addresses from text with a dummy. Solution from https://github.com/bigcode-project/bigcode-analysis/blob/main/data_analysis/pii/utils/emails_ip_addresses_detection.py#L48\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntext\n\n\nThe text to replace ip addresses in\n\n\ndummy1\nstr\n84.107.244.23\nThe dummy text to replace ipv4 addresses with\n\n\ndummy2\nstr\ndb2c:7ab5:1955:85ff:dfcd:786a:b58b:afe0\nThe dummy text to replace ipv6 addresses with\n\n\nReturns\nstr\n\nThe text with ip addresses replaced\n\n\n\n\n# test the replace_ip function\nip4_after_space = \"foo 111.222.3.4\"\nip4_before_space = \"111.222.3.4 bar\"\nip6_with_colons = \"2001:0db8:0000:0000:0000:8a2e:0370:7334\"\n\nassert replace_ip(ip4_after_space, \"127.0.0.1\") == \"foo 127.0.0.1\"\nassert replace_ip(ip4_before_space, \"127.0.0.1\") == \"127.0.0.1 bar\"\nassert replace_ip(ip6_with_colons, \"127.0.0.1\", \"0:0:0:0:0:0:0:1\") == \"0:0:0:0:0:0:0:1\"\n\n\nsource\n\n\nreplace_credit_card\n\n replace_credit_card (text:str, dummy:str='180069843287712')\n\nReplace credit card numbers from text with a dummy.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntext\nstr\n\nThe text to replace credit card numbers in\n\n\ndummy\nstr\n180069843287712\nThe dummy text to replace credit card numbers with\n\n\nReturns\nstr\n\nThe text with credit card numbers replaced\n\n\n\n\n# test the replace_credit_card function\ncredit_card_after_space = \"foo 1111-2222-3333-4444\"\ncredit_card_before_space = \"1111-2222-3333-4444 bar\"\n\nassert replace_credit_card(credit_card_after_space, \"1234-5678-9012-3456\") == \"foo 1234-5678-9012-3456\"\nassert replace_credit_card(credit_card_before_space, \"1234-5678-9012-3456\") == \"1234-5678-9012-3456 bar\"\n\n\nsource\n\n\nreplace_ssn\n\n replace_ssn (text:str, dummy:str='577-88-7519')\n\nReplace social security numbers from text with a dummy.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntext\nstr\n\nThe text to replace social security numbers in\n\n\ndummy\nstr\n577-88-7519\nThe dummy text to replace social security numbers with\n\n\nReturns\nstr\n\nThe text with social security numbers replaced\n\n\n\n\n# test the replace_ssn function\nssn_after_space = \"foo 111-22-3333\"\nssn_before_space = \"111-22-3333 bar\"\n\nassert replace_ssn(ssn_after_space, \"123-45-6789\") == \"foo 123-45-6789\"\nassert replace_ssn(ssn_before_space, \"123-45-6789\") == \"123-45-6789 bar\"\n\n\nsource\n\n\nfix_utf8_encoding\n\n fix_utf8_encoding (text:str)\n\nFix utf8 text using ftfy.\n\n\n\n\nType\nDetails\n\n\n\n\ntext\nstr\nThe text to fix\n\n\nReturns\nstr\nThe fixed text\n\n\n\n\n# test the fix_utf8_encoding function\nbad_text = 'âœ” No problems'\nassert fix_utf8_encoding(bad_text) == '✔ No problems'\nbad_text = 'dÃ©jÃ  vu'\nassert fix_utf8_encoding(bad_text) == 'déjà vu'\nbad_text = 'Ã©'\nassert fix_utf8_encoding(bad_text) == 'é'\nbad_text = 'P&EACUTE;REZ'\nassert fix_utf8_encoding(bad_text) == 'PÉREZ'"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "squeakily",
    "section": "",
    "text": "This repository is heavily inspired by BigScience’s ROOTs project and EleutherAI’s The Pile.\nThe overall pipeline is as follows:\nflowchart LR\n  A(Defining &lt;br/&gt;Datasources) --&gt; B(Defining Filters &lt;br/&gt;per Datasource)\n  B --&gt; C(Defining Cleaners &lt;br/&gt;per Datasource)\nIn this library, we define filtering as data instances being removed from the dataset based on some criteria and cleaning as data instances being modified in some way."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "squeakily",
    "section": "Install",
    "text": "Install\npip install squeakily"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "squeakily",
    "section": "How to use",
    "text": "How to use\n\nUsing the API\nFirst, we need to define a datasource. squeakily accepts any Dataset object from the HuggingFace Datasets library. For example, we can use the wikitext dataset:\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"wikitext\", \"wikitext-103-v1\", split=\"train[:1%]\")\n\nWe simply need to wrap the Dataset object in a dictionary, with the key being the name of the datasource and the value being the Dataset object, the filter and cleaners. For example:\n\nfrom squeakily.filter import check_char_repetition, check_flagged_words\nfrom squeakily.clean import remove_empty_lines, normalize_whitespace\n\ndatasources = [\n    {\n        \"dataset\": ds,\n        \"name\": \"wikitext\",\n        \"columns\": [\"text\"],\n        \"filters\": [check_char_repetition, check_flagged_words],\n        \"cleaners\": [remove_empty_lines, normalize_whitespace],\n    },\n    # ...\n]\n\n\n\n\n\n\n\nWarning\n\n\n\nNote: The order of the filters and cleaning functions matter. Filters and cleaners are applied in the order they are defined.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNote: As of now, we only use the first column of the given column names. This is because the squeakily library is designed to work with language datasets, which usually have a single column of text. Future versions will support multiple columns.\n\n\nFinally, we can apply the filters and cleaners to the datasouces using a Pipeline object:\n\nfrom squeakily.core import Pipeline\n\npipeline = Pipeline(datasources)\npipeline.run()\n\n[11/16/22 04:32:57] INFO     Running datasource: wikitext                                                core.py:41\n\n\n\n                    INFO     Running filter: check_char_repetition on text                               core.py:54\n\n\n\n                    INFO     Running filter: check_flagged_words on text                                 core.py:54\n\n\n\n                    INFO     Running cleaner: remove_empty_lines on text                                 core.py:57\n\n\n\n[11/16/22 04:32:59] INFO     Running cleaner: normalize_whitespace on text                               core.py:57\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote: If you want to run cleaners first, you can pass cleaning_first=True to the run function.\npipeline.run(cleaning_first=True)\n\n\nIf you need to run a filter or cleaner at the dataset level rather than the example level, you can pass global_filters or global_cleaners to the Pipeline.run function. For example:\n\nfrom squeakily.filter import minhash_dedup\n\npipeline.run(global_filters=[minhash_dedup])\n\n\n\n\n\n\n\nNote\n\n\n\nNote: If you use global filters or cleaners, all datasets must have a common column name in order to properly concatenate them.\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote: You can also specifiy if you want a specific dataset to be skipped by setting the skip_global parameter to True when defining the datasource.\ndatasources = [\n    {\n        \"dataset\": ds,\n        \"columns\": [\"text\"],\n        \"filters\": [check_char_repetition, check_flagged_words],\n        \"cleaners\": [remove_empty_lines, normalize_whitespace],\n        \"skip_global\": True,\n    },\n    # ...\n]\n\n\nAdditionally, you can run the pipeline in a dry run mode by passing dry_run=True to the run function. This will make no modifications to the datasets’ documents, but will add additional columns to the datasets with the results of the filters and cleaners. For example, if you if you ran the pipeline with the check_char_repetition filter, you would get a new column called check_char_repetition with a float value between 0 and 1 indicating the percentage of characters that are repeated in the document.\n\n::: {.cell}\n``` {.python .cell-code}\npipeline = Pipeline(datasources)\npipeline.run(dry_run=True)\npipeline.datasources[0][\"dataset\"].features\n:::"
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "source\n\nPipeline\n\n Pipeline (datasources)\n\nA pipeline is a collection of datasources and their associated transformations to be run.\n\n\n\n\nDetails\n\n\n\n\ndatasources\nThe datasources to be run\n\n\n\nshow_doc(Pipeline.run)\n\nsource\n\n\nPipeline.run\n\n Pipeline.run (global_filters=[], global_cleaners=[],\n               cleaning_first=False, globals_first=False, dry_run=False,\n               num_proc=2)\n\nRun the pipeline.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nglobal_filters\nlist\n[]\nFilters to be run at the dataset level rather than the example level\n\n\nglobal_cleaners\nlist\n[]\nCleaners to be run at the dataset level rather than the example level\n\n\ncleaning_first\nbool\nFalse\nWhether to run the cleaning transformations first\n\n\nglobals_first\nbool\nFalse\nWhether to run the global transformations first\n\n\ndry_run\nbool\nFalse\nWhether to run the pipeline or only calculate the various criteria and add as a column\n\n\nnum_proc\nint\n2\nNumber of processes to use\n\n\n\n\n# test dry run\nds = load_dataset(\"wikitext\", \"wikitext-103-v1\", split=\"train[:1%]\")\nlogger.info(f\"Original dataset size: {len(ds)}\")\ndatasources = [\n    {\n        \"dataset\": ds,\n        \"name\": \"wikitext\",\n        \"columns\": [\"text\"],\n        \"filters\": [],\n        \"cleaners\": [],\n    },\n    # ...\n]\n\npipeline = Pipeline(datasources)\npipeline.run(dry_run=True, global_filters=[minhash_dedup])\n\nassert len(ds) == len(pipeline.datasources[0][\"dataset\"])\nassert \"duplicate\" in pipeline.datasources[0][\"dataset\"].features\nassert \"meta_data\" in pipeline.datasources[0][\"dataset\"].features\nassert \"__id__\" in pipeline.datasources[0][\"dataset\"].features\n\n[12/02/22 05:21:47] INFO     Original dataset size: 18014                                           2007760668.py:3\n\n\n\n                    INFO     Running datasource: wikitext                                          4230721344.py:43\n\n\n\n                    INFO     Running global filter: minhash_dedup                                  4230721344.py:94\n\n\n\n                                                                                                                                                                                                                                                                                                                                                                                                \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npip install -e \".[dev]\"\n\n\n# test the ability to skip global filters\nds = load_dataset(\"wikitext\", \"wikitext-103-v1\", split=\"train[:1%]\")\nds_1 = load_dataset(\"lcama/elon-tweets\", split=\"train\")\n\ndatasources = [\n    {\n        \"dataset\": ds,\n        \"name\": \"wikitext\",\n        \"columns\": [\"text\"],\n        \"filters\": [],\n        \"cleaners\": [],\n        \"skip_global\": False,\n    },\n    {\n        \"dataset\": ds_1,\n        \"name\": \"elon\",\n        \"columns\": [\"text\"],\n        \"filters\": [],\n        \"cleaners\": [],\n        \"skip_global\": False,\n    },\n    # ...\n]\npipeline = Pipeline(datasources)\npipeline.run(global_filters=[minhash_dedup])\nlogger.info(f\"Final dataset size: {len(pipeline.datasources[0]['dataset'])}\")\n\nassert len(ds) &gt; len(pipeline.datasources[0][\"dataset\"])\nassert len(ds_1) &gt; len(pipeline.datasources[1][\"dataset\"])\n\n\n\n\nDownloading and preparing dataset None/None (download: 133.54 KiB, generated: 201.20 KiB, post-processed: Unknown size, total: 334.74 KiB) to /home/nathan/.cache/huggingface/datasets/lcama___parquet/lcama--elon-tweets-dcde4a32936ef7f2/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\nDataset parquet downloaded and prepared to /home/nathan/.cache/huggingface/datasets/lcama___parquet/lcama--elon-tweets-dcde4a32936ef7f2/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[12/02/22 05:23:00] INFO     Running datasource: wikitext                                          4230721344.py:43\n\n\n\n                    INFO     Running datasource: elon                                              4230721344.py:43\n\n\n\n                    INFO     Running global filter: minhash_dedup                                  4230721344.py:94\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[12/02/22 05:23:54] INFO     Final dataset size: 10560                                             2684403668.py:26\n\n\n\n\nlen(ds), len(ds_1), len(pipeline.datasources[0][\"dataset\"]), len(pipeline.datasources[1][\"dataset\"])\n\n(18014, 1601, 10560, 1597)\n\n\n\ndef formatted(text):\n    \"\"\"\n    Format a question and answer pair for training a decoder-only model.\n    \"\"\"\n    formatted_example = f\"\"\"\n    This tweet was tweeted by Elon Musk.\n    Here it is: {text}\n    \"\"\"\n    return formatted_example\n\nfake_qa = ds_1.map(\n    lambda x: {\"text\": formatted(x[\"text\"])}\n)\ndatasources = [\n    {\n        \"dataset\": ds,\n        \"name\": \"wikitext\",\n        \"columns\": [\"text\"],\n        \"filters\": [],\n        \"cleaners\": [],\n        \"skip_global\": False,\n    },\n    {\n        \"dataset\": fake_qa,\n        \"name\": \"elon_qa\",\n        \"columns\": [\"text\"],\n        \"filters\": [],\n        \"cleaners\": [],\n        \"skip_global\": False,\n    },\n    # ...\n]\npipeline = Pipeline(datasources)\npipeline.run(global_filters=[minhash_dedup])\nlogger.info(f\"Final dataset size: {len(pipeline.datasources[0]['dataset'])}\")\n\nassert len(ds) &gt; len(pipeline.datasources[0][\"dataset\"])\nassert len(fake_qa) &gt; len(pipeline.datasources[1][\"dataset\"])\n\n\n\n\n[12/02/22 05:24:00] INFO     Running datasource: wikitext                                          4230721344.py:43\n\n\n\n                    INFO     Running datasource: elon_qa                                           4230721344.py:43\n\n\n\n                    INFO     Running global filter: minhash_dedup                                  4230721344.py:94\n\n\n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[12/02/22 05:24:55] INFO     Final dataset size: 10560                                             1197017855.py:35\n\n\n\n\nlen(ds_1), len(fake_qa), len(pipeline.datasources[0][\"dataset\"]), len(pipeline.datasources[1][\"dataset\"])\n\n(1601, 1601, 10560, 1589)\n\n\n\n# test dry run\nds = load_dataset(\"wikitext\", \"wikitext-103-v1\", split=\"train[:1%]\")\nlogger.info(f\"Original dataset size: {len(ds)}\")\ndatasources = [\n    {\n        \"dataset\": ds,\n        \"name\": \"wikitext\",\n        \"columns\": [\"text\"],\n        \"filters\": [check_char_repetition, check_flagged_words],\n        \"cleaners\": [remove_empty_lines, normalize_whitespace],\n    },\n    # ...\n]\n\npipeline = Pipeline(datasources)\npipeline.run(dry_run=True, global_filters=[minhash_dedup])\n\nassert len(ds) == len(pipeline.datasources[0][\"dataset\"])\nassert \"check_char_repetition_criteria\" in pipeline.datasources[0][\"dataset\"].features\nassert \"check_flagged_words_criteria\" in pipeline.datasources[0][\"dataset\"].features\nassert \"duplicate\" in pipeline.datasources[0][\"dataset\"].features\nassert \"meta_data\" in pipeline.datasources[0][\"dataset\"].features\nassert \"__id__\" in pipeline.datasources[0][\"dataset\"].features\n\n[12/02/22 05:26:34] INFO     Original dataset size: 18014                                           4207204728.py:3\n\n\n\n                    INFO     Running datasource: wikitext                                          4230721344.py:43\n\n\n\n                    INFO     Running filter: check_char_repetition on text                         4230721344.py:17\n\n\n\n                    INFO     Running in dry-run mode                                               4230721344.py:19\n\n\n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                    INFO     Running filter: check_flagged_words on text                           4230721344.py:17\n\n\n\n                    INFO     Running in dry-run mode                                               4230721344.py:19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[12/02/22 05:26:35] INFO     Running cleaner: remove_empty_lines on text                           4230721344.py:69\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[12/02/22 05:26:36] INFO     Running cleaner: normalize_whitespace on text                         4230721344.py:69\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[12/02/22 05:26:37] INFO     Running global filter: minhash_dedup                                  4230721344.py:94\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# test dry run with partials\nfrom functools import partial\n\nds = load_dataset(\"wikitext\", \"wikitext-103-v1\", split=\"train[:1%]\")\nlogger.info(f\"Original dataset size: {len(ds)}\")\ncheck_char_repetition_p = partial(check_char_repetition, char_repetition_len=3)\ncheck_char_repetition_p.__name__ = \"check_char_repetition\"\ndatasources = [\n    {\n        \"dataset\": ds,\n        \"name\": \"wikitext\",\n        \"columns\": [\"text\"],\n        \"filters\": [check_char_repetition_p, check_flagged_words],\n        \"cleaners\": [remove_empty_lines, normalize_whitespace],\n    },\n    # ...\n]\n\npipeline = Pipeline(datasources)\npipeline.run(dry_run=True)\n\nassert len(ds) == len(pipeline.datasources[0][\"dataset\"])\nassert \"check_char_repetition_criteria\" in pipeline.datasources[0][\"dataset\"].features\nassert \"check_flagged_words_criteria\" in pipeline.datasources[0][\"dataset\"].features\n\nDownloading and preparing dataset wikitext/wikitext-103-v1 to /home/runner/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126...\nDataset wikitext downloaded and prepared to /home/runner/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126. Subsequent calls will reuse this data.\n[06/16/23 22:16:16] INFO     Original dataset   &lt;ipython-input-1-b7aa22f09144&gt;:5\n                             size: 18014                                        \n                    INFO     Running           &lt;ipython-input-1-afbdab7dc7e6&gt;:43\n                             datasource:                                        \n                             wikitext                                           \n                    INFO     Running filter:   &lt;ipython-input-1-afbdab7dc7e6&gt;:16\n                             check_char_repeti                                  \n                             tion on text                                       \n                    INFO     Running in        &lt;ipython-input-1-afbdab7dc7e6&gt;:18\n                             dry-run mode                                       \n[06/16/23 22:16:18] INFO     Running filter:   &lt;ipython-input-1-afbdab7dc7e6&gt;:16\n                             check_flagged_wor                                  \n                             ds on text                                         \n                    INFO     Running in        &lt;ipython-input-1-afbdab7dc7e6&gt;:18\n                             dry-run mode                                       \n[06/16/23 22:16:19] INFO     Running cleaner:  &lt;ipython-input-1-afbdab7dc7e6&gt;:71\n                             remove_empty_line                                  \n                             s on text                                          \n                    INFO     Running cleaner:  &lt;ipython-input-1-afbdab7dc7e6&gt;:71\n                             normalize_whitesp                                  \n                             ace on text                                        \n\n\nDownloading builder script:   0%|          | 0.00/8.48k [00:00&lt;?, ?B/s]Downloading builder script: 100%|##########| 8.48k/8.48k [00:00&lt;00:00, 23.5MB/s]\nDownloading metadata:   0%|          | 0.00/6.84k [00:00&lt;?, ?B/s]Downloading metadata: 100%|##########| 6.84k/6.84k [00:00&lt;00:00, 21.9MB/s]\nDownloading readme:   0%|          | 0.00/9.25k [00:00&lt;?, ?B/s]Downloading readme: 100%|##########| 9.25k/9.25k [00:00&lt;00:00, 26.6MB/s]\nDownloading data:   0%|          | 0.00/190M [00:00&lt;?, ?B/s]Downloading data:   0%|          | 121k/190M [00:00&lt;02:53, 1.10MB/s]Downloading data:   1%|          | 1.11M/190M [00:00&lt;00:32, 5.74MB/s]Downloading data:   3%|3         | 6.35M/190M [00:00&lt;00:07, 24.8MB/s]Downloading data:   6%|6         | 12.3M/190M [00:00&lt;00:04, 37.9MB/s]Downloading data:   9%|9         | 17.8M/190M [00:00&lt;00:03, 43.6MB/s]Downloading data:  12%|#2        | 23.2M/190M [00:00&lt;00:03, 47.0MB/s]Downloading data:  15%|#4        | 28.5M/190M [00:00&lt;00:03, 47.2MB/s]Downloading data:  18%|#8        | 34.7M/190M [00:00&lt;00:03, 51.8MB/s]Downloading data:  21%|##1       | 40.6M/190M [00:00&lt;00:02, 54.0MB/s]Downloading data:  24%|##4       | 46.0M/190M [00:01&lt;00:02, 50.2MB/s]Downloading data:  28%|##7       | 52.3M/190M [00:01&lt;00:02, 53.8MB/s]Downloading data:  31%|###       | 58.1M/190M [00:01&lt;00:02, 54.8MB/s]Downloading data:  33%|###3      | 63.6M/190M [00:01&lt;00:02, 55.0MB/s]Downloading data:  36%|###6      | 69.1M/190M [00:01&lt;00:02, 50.8MB/s]Downloading data:  39%|###9      | 74.3M/190M [00:01&lt;00:02, 50.5MB/s]Downloading data:  42%|####1     | 79.4M/190M [00:01&lt;00:02, 46.5MB/s]Downloading data:  44%|####4     | 84.2M/190M [00:01&lt;00:02, 45.2MB/s]Downloading data:  47%|####6     | 88.8M/190M [00:01&lt;00:02, 44.1MB/s]Downloading data:  49%|####8     | 93.2M/190M [00:02&lt;00:02, 43.3MB/s]Downloading data:  51%|#####1    | 97.6M/190M [00:02&lt;00:02, 42.2MB/s]Downloading data:  54%|#####3    | 102M/190M [00:02&lt;00:02, 41.6MB/s] Downloading data:  56%|#####5    | 106M/190M [00:02&lt;00:02, 39.0MB/s]Downloading data:  58%|#####7    | 110M/190M [00:02&lt;00:02, 36.5MB/s]Downloading data:  60%|#####9    | 114M/190M [00:02&lt;00:02, 33.9MB/s]Downloading data:  62%|######1   | 117M/190M [00:02&lt;00:02, 33.1MB/s]Downloading data:  63%|######3   | 120M/190M [00:02&lt;00:02, 32.4MB/s]Downloading data:  65%|######5   | 124M/190M [00:02&lt;00:02, 32.2MB/s]Downloading data:  67%|######6   | 127M/190M [00:03&lt;00:01, 32.6MB/s]Downloading data:  69%|######8   | 130M/190M [00:03&lt;00:01, 32.5MB/s]Downloading data:  70%|#######   | 134M/190M [00:03&lt;00:01, 31.9MB/s]Downloading data:  72%|#######1  | 137M/190M [00:03&lt;00:01, 31.7MB/s]Downloading data:  74%|#######3  | 140M/190M [00:03&lt;00:01, 32.0MB/s]Downloading data:  76%|#######5  | 144M/190M [00:03&lt;00:01, 32.9MB/s]Downloading data:  77%|#######7  | 147M/190M [00:03&lt;00:01, 33.2MB/s]Downloading data:  79%|#######9  | 150M/190M [00:03&lt;00:01, 32.8MB/s]Downloading data:  81%|########  | 154M/190M [00:03&lt;00:01, 33.0MB/s]Downloading data:  83%|########2 | 157M/190M [00:04&lt;00:00, 33.3MB/s]Downloading data:  85%|########4 | 161M/190M [00:04&lt;00:00, 34.1MB/s]Downloading data:  86%|########6 | 164M/190M [00:04&lt;00:00, 33.9MB/s]Downloading data:  88%|########8 | 168M/190M [00:04&lt;00:00, 33.5MB/s]Downloading data:  90%|########9 | 171M/190M [00:04&lt;00:00, 33.6MB/s]Downloading data:  92%|#########1| 175M/190M [00:04&lt;00:00, 34.3MB/s]Downloading data:  94%|#########3| 178M/190M [00:04&lt;00:00, 32.1MB/s]Downloading data:  95%|#########5| 181M/190M [00:04&lt;00:00, 30.5MB/s]Downloading data:  97%|#########6| 184M/190M [00:04&lt;00:00, 28.5MB/s]Downloading data:  98%|#########8| 187M/190M [00:04&lt;00:00, 27.3MB/s]Downloading data: 100%|#########9| 190M/190M [00:05&lt;00:00, 26.7MB/s]Downloading data: 100%|##########| 190M/190M [00:05&lt;00:00, 37.2MB/s]\nGenerating test split:   0%|          | 0/4358 [00:00&lt;?, ? examples/s]                                                                      Generating train split:   0%|          | 0/1801350 [00:00&lt;?, ? examples/s]Generating train split:   0%|          | 6996/1801350 [00:00&lt;00:25, 69841.18 examples/s]Generating train split:   1%|          | 14000/1801350 [00:00&lt;00:25, 69597.68 examples/s]Generating train split:   1%|1         | 21020/1801350 [00:00&lt;00:25, 69867.70 examples/s]Generating train split:   2%|1         | 28050/1801350 [00:00&lt;00:25, 70034.57 examples/s]Generating train split:   2%|2         | 38579/1801350 [00:00&lt;00:25, 70104.47 examples/s]Generating train split:   3%|2         | 45605/1801350 [00:00&lt;00:25, 70148.46 examples/s]Generating train split:   3%|2         | 52729/1801350 [00:00&lt;00:24, 70475.42 examples/s]Generating train split:   4%|3         | 63247/1801350 [00:00&lt;00:24, 70330.83 examples/s]Generating train split:   4%|3         | 70285/1801350 [00:01&lt;00:24, 70341.83 examples/s]Generating train split:   4%|4         | 77350/1801350 [00:01&lt;00:24, 70424.90 examples/s]Generating train split:   5%|4         | 87885/1801350 [00:01&lt;00:24, 70351.46 examples/s]Generating train split:   5%|5         | 98384/1801350 [00:01&lt;00:24, 70221.82 examples/s]Generating train split:   6%|5         | 105431/1801350 [00:01&lt;00:24, 70282.23 examples/s]Generating train split:   6%|6         | 112464/1801350 [00:01&lt;00:24, 70293.78 examples/s]Generating train split:   7%|6         | 122970/1801350 [00:01&lt;00:23, 70197.46 examples/s]Generating train split:   7%|7         | 130000/1801350 [00:01&lt;00:23, 70104.82 examples/s]Generating train split:   8%|7         | 137064/1801350 [00:01&lt;00:23, 70246.30 examples/s]Generating train split:   8%|8         | 147562/1801350 [00:02&lt;00:23, 70148.67 examples/s]Generating train split:   9%|8         | 158085/1801350 [00:02&lt;00:23, 70114.15 examples/s]Generating train split:   9%|9         | 165122/1801350 [00:02&lt;00:23, 70175.12 examples/s]Generating train split:  10%|9         | 175697/1801350 [00:02&lt;00:23, 70282.28 examples/s]Generating train split:  10%|#         | 186134/1801350 [00:02&lt;00:23, 70047.75 examples/s]Generating train split:  11%|#         | 193186/1801350 [00:02&lt;00:22, 70158.49 examples/s]Generating train split:  11%|#1        | 200208/1801350 [00:02&lt;00:22, 70171.36 examples/s]Generating train split:  12%|#1        | 210683/1801350 [00:03&lt;00:22, 70048.64 examples/s]Generating train split:  12%|#2        | 221121/1801350 [00:03&lt;00:22, 69891.15 examples/s]Generating train split:  13%|#2        | 228127/1801350 [00:03&lt;00:22, 69928.92 examples/s]Generating train split:  13%|#3        | 235174/1801350 [00:03&lt;00:22, 70064.93 examples/s]Generating train split:  13%|#3        | 243163/1801350 [00:03&lt;00:25, 61640.40 examples/s]Generating train split:  14%|#3        | 250235/1801350 [00:03&lt;00:24, 63865.24 examples/s]Generating train split:  14%|#4        | 257232/1801350 [00:03&lt;00:23, 65447.21 examples/s]Generating train split:  15%|#4        | 264232/1801350 [00:03&lt;00:23, 66675.85 examples/s]Generating train split:  15%|#5        | 271231/1801350 [00:03&lt;00:22, 67596.72 examples/s]Generating train split:  15%|#5        | 278266/1801350 [00:04&lt;00:22, 68377.92 examples/s]Generating train split:  16%|#5        | 285253/1801350 [00:04&lt;00:22, 68808.05 examples/s]Generating train split:  16%|#6        | 292224/1801350 [00:04&lt;00:21, 69069.23 examples/s]Generating train split:  17%|#6        | 299275/1801350 [00:04&lt;00:21, 69489.90 examples/s]Generating train split:  17%|#7        | 306297/1801350 [00:04&lt;00:21, 69703.40 examples/s]Generating train split:  17%|#7        | 313344/1801350 [00:04&lt;00:21, 69929.99 examples/s]Generating train split:  18%|#7        | 323832/1801350 [00:04&lt;00:21, 69922.28 examples/s]Generating train split:  18%|#8        | 330845/1801350 [00:04&lt;00:21, 69977.74 examples/s]Generating train split:  19%|#8        | 337871/1801350 [00:04&lt;00:20, 70054.45 examples/s]Generating train split:  19%|#9        | 348233/1801350 [00:05&lt;00:20, 69685.37 examples/s]Generating train split:  20%|#9        | 358689/1801350 [00:05&lt;00:20, 69690.23 examples/s]Generating train split:  20%|##        | 365747/1801350 [00:05&lt;00:20, 69909.92 examples/s]Generating train split:  21%|##        | 372810/1801350 [00:05&lt;00:20, 70094.91 examples/s]Generating train split:  21%|##1       | 379860/1801350 [00:05&lt;00:20, 70201.53 examples/s]Generating train split:  21%|##1       | 386950/1801350 [00:05&lt;00:20, 70396.05 examples/s]Generating train split:  22%|##1       | 394000/1801350 [00:05&lt;00:20, 70214.28 examples/s]Generating train split:  22%|##2       | 401050/1801350 [00:05&lt;00:19, 70294.70 examples/s]Generating train split:  23%|##2       | 411597/1801350 [00:05&lt;00:19, 70295.83 examples/s]Generating train split:  23%|##3       | 422000/1801350 [00:06&lt;00:19, 69858.97 examples/s]Generating train split:  24%|##3       | 429008/1801350 [00:06&lt;00:19, 69912.37 examples/s]Generating train split:  24%|##4       | 436035/1801350 [00:06&lt;00:19, 70001.73 examples/s]Generating train split:  25%|##4       | 443089/1801350 [00:06&lt;00:19, 70146.41 examples/s]Generating train split:  25%|##5       | 453632/1801350 [00:06&lt;00:19, 70194.83 examples/s]Generating train split:  26%|##5       | 460669/1801350 [00:06&lt;00:19, 70237.83 examples/s]Generating train split:  26%|##6       | 471149/1801350 [00:06&lt;00:18, 70100.75 examples/s]Generating train split:  27%|##6       | 478190/1801350 [00:06&lt;00:18, 70178.13 examples/s]Generating train split:  27%|##6       | 485244/1801350 [00:06&lt;00:18, 70273.22 examples/s]Generating train split:  27%|##7       | 492297/1801350 [00:07&lt;00:18, 70340.81 examples/s]Generating train split:  28%|##7       | 499349/1801350 [00:07&lt;00:18, 70389.43 examples/s]Generating train split:  28%|##8       | 509906/1801350 [00:07&lt;00:18, 70383.07 examples/s]Generating train split:  29%|##8       | 520416/1801350 [00:07&lt;00:18, 70267.97 examples/s]Generating train split:  29%|##9       | 530980/1801350 [00:07&lt;00:18, 70317.33 examples/s]Generating train split:  30%|###       | 541414/1801350 [00:07&lt;00:17, 70070.76 examples/s]Generating train split:  30%|###       | 548456/1801350 [00:07&lt;00:17, 70150.06 examples/s]Generating train split:  31%|###       | 555552/1801350 [00:07&lt;00:17, 70190.12 examples/s]Generating train split:  31%|###1      | 562587/1801350 [00:08&lt;00:17, 70228.29 examples/s]Generating train split:  32%|###1      | 573000/1801350 [00:08&lt;00:17, 69922.40 examples/s]Generating train split:  32%|###2      | 580002/1801350 [00:08&lt;00:17, 69940.44 examples/s]Generating train split:  33%|###2      | 587071/1801350 [00:08&lt;00:17, 70138.57 examples/s]Generating train split:  33%|###3      | 597605/1801350 [00:08&lt;00:17, 70169.40 examples/s]Generating train split:  34%|###3      | 608093/1801350 [00:08&lt;00:17, 70081.48 examples/s]Generating train split:  34%|###4      | 615105/1801350 [00:08&lt;00:16, 70088.25 examples/s]Generating train split:  35%|###4      | 622165/1801350 [00:08&lt;00:16, 70219.56 examples/s]Generating train split:  35%|###5      | 632698/1801350 [00:09&lt;00:16, 70211.60 examples/s]Generating train split:  36%|###5      | 639737/1801350 [00:09&lt;00:16, 70255.25 examples/s]Generating train split:  36%|###6      | 650144/1801350 [00:09&lt;00:16, 69943.92 examples/s]Generating train split:  36%|###6      | 657205/1801350 [00:09&lt;00:16, 70110.77 examples/s]Generating train split:  37%|###6      | 664260/1801350 [00:09&lt;00:16, 70226.00 examples/s]Generating train split:  37%|###7      | 671340/1801350 [00:09&lt;00:16, 70382.61 examples/s]Generating train split:  38%|###7      | 678393/1801350 [00:09&lt;00:15, 70421.16 examples/s]Generating train split:  38%|###8      | 688862/1801350 [00:09&lt;00:15, 70181.12 examples/s]Generating train split:  39%|###8      | 699298/1801350 [00:10&lt;00:15, 69965.08 examples/s]Generating train split:  39%|###9      | 709785/1801350 [00:10&lt;00:15, 69944.32 examples/s]Generating train split:  40%|###9      | 716820/1801350 [00:10&lt;00:15, 70039.60 examples/s]Generating train split:  40%|####      | 723867/1801350 [00:10&lt;00:15, 70149.12 examples/s]Generating train split:  41%|####      | 730932/1801350 [00:10&lt;00:15, 70280.59 examples/s]Generating train split:  41%|####      | 738000/1801350 [00:10&lt;00:15, 70174.76 examples/s]Generating train split:  41%|####1     | 745039/1801350 [00:10&lt;00:15, 70233.87 examples/s]Generating train split:  42%|####1     | 752082/1801350 [00:10&lt;00:14, 70288.03 examples/s]Generating train split:  42%|####2     | 762670/1801350 [00:10&lt;00:14, 70398.43 examples/s]Generating train split:  43%|####2     | 773147/1801350 [00:11&lt;00:14, 70200.58 examples/s]Generating train split:  43%|####3     | 780174/1801350 [00:11&lt;00:14, 70216.12 examples/s]Generating train split:  44%|####3     | 790629/1801350 [00:11&lt;00:14, 70031.16 examples/s]Generating train split:  44%|####4     | 801119/1801350 [00:11&lt;00:14, 69994.71 examples/s]Generating train split:  45%|####4     | 808131/1801350 [00:11&lt;00:14, 70022.79 examples/s]Generating train split:  45%|####5     | 815199/1801350 [00:11&lt;00:14, 70189.61 examples/s]Generating train split:  46%|####5     | 822263/1801350 [00:11&lt;00:13, 70308.83 examples/s]Generating train split:  46%|####6     | 832768/1801350 [00:11&lt;00:13, 70203.92 examples/s]Generating train split:  47%|####6     | 839806/1801350 [00:12&lt;00:13, 70246.60 examples/s]Generating train split:  47%|####7     | 850278/1801350 [00:12&lt;00:13, 70086.40 examples/s]Generating train split:  48%|####7     | 860846/1801350 [00:12&lt;00:13, 70207.48 examples/s]Generating train split:  48%|####8     | 867885/1801350 [00:12&lt;00:13, 70249.60 examples/s]Generating train split:  49%|####8     | 874958/1801350 [00:12&lt;00:13, 70370.23 examples/s]Generating train split:  49%|####8     | 882000/1801350 [00:12&lt;00:13, 70148.95 examples/s]Generating train split:  50%|####9     | 892584/1801350 [00:12&lt;00:12, 70295.87 examples/s]Generating train split:  50%|#####     | 903053/1801350 [00:12&lt;00:12, 70120.53 examples/s]Generating train split:  51%|#####     | 913592/1801350 [00:13&lt;00:12, 70165.28 examples/s]Generating train split:  51%|#####1    | 920625/1801350 [00:13&lt;00:12, 70202.81 examples/s]Generating train split:  51%|#####1    | 927649/1801350 [00:13&lt;00:12, 70208.66 examples/s]Generating train split:  52%|#####1    | 934687/1801350 [00:13&lt;00:12, 70252.16 examples/s]Generating train split:  52%|#####2    | 945118/1801350 [00:13&lt;00:12, 69989.96 examples/s]Generating train split:  53%|#####3    | 954721/1801350 [00:13&lt;00:13, 62433.26 examples/s]Generating train split:  53%|#####3    | 961790/1801350 [00:13&lt;00:13, 64332.41 examples/s]Generating train split:  54%|#####3    | 968807/1801350 [00:13&lt;00:12, 65773.45 examples/s]Generating train split:  54%|#####4    | 975796/1801350 [00:14&lt;00:12, 66846.79 examples/s]Generating train split:  55%|#####4    | 982807/1801350 [00:14&lt;00:12, 67732.37 examples/s]Generating train split:  55%|#####4    | 989793/1801350 [00:14&lt;00:11, 68325.61 examples/s]Generating train split:  55%|#####5    | 996866/1801350 [00:14&lt;00:11, 69009.51 examples/s]Generating train split:  56%|#####5    | 1003892/1801350 [00:14&lt;00:11, 69367.07 examples/s]Generating train split:  56%|#####6    | 1010944/1801350 [00:14&lt;00:11, 69700.47 examples/s]Generating train split:  57%|#####6    | 1017989/1801350 [00:14&lt;00:11, 69919.39 examples/s]Generating train split:  57%|#####7    | 1028536/1801350 [00:14&lt;00:11, 69873.50 examples/s]Generating train split:  57%|#####7    | 1035594/1801350 [00:14&lt;00:10, 70029.83 examples/s]Generating train split:  58%|#####7    | 1042635/1801350 [00:14&lt;00:10, 70132.84 examples/s]Generating train split:  58%|#####8    | 1053076/1801350 [00:15&lt;00:10, 69932.61 examples/s]Generating train split:  59%|#####9    | 1063649/1801350 [00:15&lt;00:10, 70122.18 examples/s]Generating train split:  59%|#####9    | 1070689/1801350 [00:15&lt;00:10, 70188.48 examples/s]Generating train split:  60%|######    | 1081157/1801350 [00:15&lt;00:10, 70046.94 examples/s]Generating train split:  60%|######    | 1088195/1801350 [00:15&lt;00:10, 70128.88 examples/s]Generating train split:  61%|######    | 1098741/1801350 [00:15&lt;00:10, 70188.61 examples/s]Generating train split:  62%|######1   | 1109227/1801350 [00:15&lt;00:09, 70091.20 examples/s]Generating train split:  62%|######2   | 1119819/1801350 [00:16&lt;00:09, 70257.69 examples/s]Generating train split:  63%|######2   | 1126902/1801350 [00:16&lt;00:09, 70390.16 examples/s]Generating train split:  63%|######2   | 1133951/1801350 [00:16&lt;00:09, 70413.75 examples/s]Generating train split:  64%|######3   | 1144453/1801350 [00:16&lt;00:09, 70271.93 examples/s]Generating train split:  64%|######4   | 1155000/1801350 [00:16&lt;00:09, 70100.43 examples/s]Generating train split:  65%|######4   | 1165574/1801350 [00:16&lt;00:09, 70224.90 examples/s]Generating train split:  65%|######5   | 1172625/1801350 [00:16&lt;00:08, 70290.50 examples/s]Generating train split:  65%|######5   | 1179666/1801350 [00:16&lt;00:08, 70317.65 examples/s]Generating train split:  66%|######6   | 1190090/1801350 [00:17&lt;00:08, 70028.40 examples/s]Generating train split:  67%|######6   | 1200687/1801350 [00:17&lt;00:08, 70230.21 examples/s]Generating train split:  67%|######7   | 1207743/1801350 [00:17&lt;00:08, 70305.06 examples/s]Generating train split:  67%|######7   | 1214825/1801350 [00:17&lt;00:08, 70434.32 examples/s]Generating train split:  68%|######7   | 1221942/1801350 [00:17&lt;00:08, 70627.81 examples/s]Generating train split:  68%|######8   | 1232601/1801350 [00:17&lt;00:08, 70618.68 examples/s]Generating train split:  69%|######9   | 1243082/1801350 [00:17&lt;00:07, 70360.41 examples/s]Generating train split:  69%|######9   | 1250140/1801350 [00:17&lt;00:07, 70412.01 examples/s]Generating train split:  70%|######9   | 1260727/1801350 [00:18&lt;00:07, 70465.02 examples/s]Generating train split:  70%|#######   | 1267785/1801350 [00:18&lt;00:07, 70490.60 examples/s]Generating train split:  71%|#######   | 1278264/1801350 [00:18&lt;00:07, 70269.08 examples/s]Generating train split:  71%|#######1  | 1285369/1801350 [00:18&lt;00:07, 70461.18 examples/s]Generating train split:  72%|#######1  | 1295989/1801350 [00:18&lt;00:07, 70575.04 examples/s]Generating train split:  73%|#######2  | 1306556/1801350 [00:18&lt;00:07, 70412.01 examples/s]Generating train split:  73%|#######3  | 1317000/1801350 [00:18&lt;00:06, 70042.67 examples/s]Generating train split:  74%|#######3  | 1327515/1801350 [00:19&lt;00:06, 69927.34 examples/s]Generating train split:  74%|#######4  | 1334553/1801350 [00:19&lt;00:06, 69990.67 examples/s]Generating train split:  74%|#######4  | 1341608/1801350 [00:19&lt;00:06, 70127.45 examples/s]Generating train split:  75%|#######4  | 1348643/1801350 [00:19&lt;00:06, 70182.39 examples/s]Generating train split:  75%|#######5  | 1359074/1801350 [00:19&lt;00:06, 69948.64 examples/s]Generating train split:  76%|#######6  | 1369599/1801350 [00:19&lt;00:06, 70020.29 examples/s]Generating train split:  77%|#######6  | 1380000/1801350 [00:19&lt;00:06, 69658.17 examples/s]Generating train split:  77%|#######6  | 1387000/1801350 [00:19&lt;00:05, 69656.20 examples/s]Generating train split:  77%|#######7  | 1394000/1801350 [00:19&lt;00:05, 69675.53 examples/s]Generating train split:  78%|#######7  | 1401000/1801350 [00:20&lt;00:05, 69709.01 examples/s]Generating train split:  78%|#######8  | 1408000/1801350 [00:20&lt;00:05, 69760.47 examples/s]Generating train split:  79%|#######8  | 1415000/1801350 [00:20&lt;00:05, 69782.21 examples/s]Generating train split:  79%|#######8  | 1422000/1801350 [00:20&lt;00:05, 69840.70 examples/s]Generating train split:  79%|#######9  | 1429059/1801350 [00:20&lt;00:05, 70056.51 examples/s]Generating train split:  80%|#######9  | 1436161/1801350 [00:20&lt;00:05, 70337.25 examples/s]Generating train split:  80%|########  | 1443206/1801350 [00:20&lt;00:05, 70366.05 examples/s]Generating train split:  81%|########  | 1453720/1801350 [00:20&lt;00:04, 70258.49 examples/s]Generating train split:  81%|########1 | 1464175/1801350 [00:20&lt;00:04, 70054.95 examples/s]Generating train split:  82%|########1 | 1474674/1801350 [00:21&lt;00:04, 70031.22 examples/s]Generating train split:  82%|########2 | 1485094/1801350 [00:21&lt;00:04, 69846.62 examples/s]Generating train split:  83%|########2 | 1492134/1801350 [00:21&lt;00:04, 69974.96 examples/s]Generating train split:  83%|########3 | 1499181/1801350 [00:21&lt;00:04, 70098.41 examples/s]Generating train split:  84%|########3 | 1509770/1801350 [00:21&lt;00:04, 70264.63 examples/s]Generating train split:  84%|########4 | 1520199/1801350 [00:21&lt;00:04, 70016.76 examples/s]Generating train split:  85%|########4 | 1527211/1801350 [00:21&lt;00:03, 70038.22 examples/s]Generating train split:  85%|########5 | 1537721/1801350 [00:22&lt;00:03, 70045.74 examples/s]Generating train split:  86%|########5 | 1544747/1801350 [00:22&lt;00:03, 70094.41 examples/s]Generating train split:  86%|########6 | 1551835/1801350 [00:22&lt;00:03, 70294.87 examples/s]Generating train split:  87%|########6 | 1562246/1801350 [00:22&lt;00:03, 69974.32 examples/s]Generating train split:  87%|########7 | 1569334/1801350 [00:22&lt;00:03, 70202.64 examples/s]Generating train split:  88%|########7 | 1579911/1801350 [00:22&lt;00:03, 70310.10 examples/s]Generating train split:  88%|########8 | 1590324/1801350 [00:22&lt;00:03, 70010.28 examples/s]Generating train split:  89%|########8 | 1600876/1801350 [00:22&lt;00:02, 70115.92 examples/s]Generating train split:  89%|########9 | 1611271/1801350 [00:23&lt;00:02, 69853.32 examples/s]Generating train split:  90%|########9 | 1618278/1801350 [00:23&lt;00:02, 69899.53 examples/s]Generating train split:  90%|######### | 1625321/1801350 [00:23&lt;00:02, 70028.87 examples/s]Generating train split:  91%|######### | 1635897/1801350 [00:23&lt;00:02, 70188.94 examples/s]Generating train split:  91%|#########1| 1642938/1801350 [00:23&lt;00:02, 70242.61 examples/s]Generating train split:  92%|#########1| 1653417/1801350 [00:23&lt;00:02, 70107.15 examples/s]Generating train split:  92%|#########2| 1662727/1801350 [00:23&lt;00:02, 62465.60 examples/s]Generating train split:  93%|#########2| 1669782/1801350 [00:23&lt;00:02, 64293.41 examples/s]Generating train split:  93%|#########3| 1676732/1801350 [00:24&lt;00:01, 65564.05 examples/s]Generating train split:  93%|#########3| 1683705/1801350 [00:24&lt;00:01, 66640.80 examples/s]Generating train split:  94%|#########3| 1690666/1801350 [00:24&lt;00:01, 67441.49 examples/s]Generating train split:  94%|#########4| 1697637/1801350 [00:24&lt;00:01, 68068.33 examples/s]Generating train split:  95%|#########4| 1704576/1801350 [00:24&lt;00:01, 68443.35 examples/s]Generating train split:  95%|#########5| 1711608/1801350 [00:24&lt;00:01, 68984.17 examples/s]Generating train split:  95%|#########5| 1718688/1801350 [00:24&lt;00:01, 69512.23 examples/s]Generating train split:  96%|#########5| 1729094/1801350 [00:24&lt;00:01, 69454.83 examples/s]Generating train split:  96%|#########6| 1736172/1801350 [00:24&lt;00:00, 69808.45 examples/s]Generating train split:  97%|#########6| 1746673/1801350 [00:25&lt;00:00, 69878.36 examples/s]Generating train split:  98%|#########7| 1757121/1801350 [00:25&lt;00:00, 69797.72 examples/s]Generating train split:  98%|#########7| 1764118/1801350 [00:25&lt;00:00, 69837.67 examples/s]Generating train split:  99%|#########8| 1774687/1801350 [00:25&lt;00:00, 70048.59 examples/s]Generating train split:  99%|#########9| 1785156/1801350 [00:25&lt;00:00, 69961.84 examples/s]Generating train split:  99%|#########9| 1792190/1801350 [00:25&lt;00:00, 70050.59 examples/s]Generating train split: 100%|##########| 1801350/1801350 [00:25&lt;00:00, 70040.93 examples/s]                                                                                           Generating validation split:   0%|          | 0/3760 [00:00&lt;?, ? examples/s]                                                                            \n\n\n\nds = load_dataset(\"wikitext\", \"wikitext-103-v1\", split=\"train[:1%]\")\nlogger.info(f\"Original dataset size: {len(ds)}\")\ndatasources = [\n    {\n        \"dataset\": ds,\n        \"name\": \"wikitext\",\n        \"columns\": [\"text\"],\n        \"filters\": [check_char_repetition, check_flagged_words],\n        \"cleaners\": [remove_empty_lines, normalize_whitespace],\n    },\n    # ...\n]\n\nglobal_filters = [minhash_dedup]\npipeline = Pipeline(datasources)\npipeline.run(global_filters=global_filters)\nlogger.info(f\"Final dataset size: {len(pipeline.datasources[0]['dataset'])}\")\n\nassert len(ds) &gt; len(pipeline.datasources[0][\"dataset\"])\n\n[12/02/22 05:27:36] INFO     Original dataset size: 18014                                           2624608473.py:2\n\n\n\n                    INFO     Running datasource: wikitext                                          4230721344.py:43\n\n\n\n                    INFO     Running filter: check_char_repetition on text                         4230721344.py:17\n\n\n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[12/02/22 05:27:37] INFO     Running filter: check_flagged_words on text                           4230721344.py:17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[12/02/22 05:27:38] INFO     Running cleaner: remove_empty_lines on text                           4230721344.py:69\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[12/02/22 05:27:39] INFO     Running cleaner: normalize_whitespace on text                         4230721344.py:69\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[12/02/22 05:27:40] INFO     Running global filter: minhash_dedup                                  4230721344.py:94\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[12/02/22 05:28:32] INFO     Final dataset size: 10557                                             2624608473.py:17\n\n\n\n\n# test the ability to skip global filters\nds = load_dataset(\"wikitext\", \"wikitext-103-v1\", split=\"train[:1%]\")\nds_1 = load_dataset(\"lcama/elon-tweets\", split=\"train\")\n\ndatasources = [\n    {\n        \"dataset\": ds,\n        \"name\": \"wikitext\",\n        \"columns\": [\"text\"],\n        \"filters\": [check_char_repetition, check_flagged_words],\n        \"cleaners\": [remove_empty_lines, normalize_whitespace],\n        \"skip_global\": False,\n    },\n    {\n        \"dataset\": ds_1,\n        \"name\": \"elon\",\n        \"columns\": [\"text\"],\n        \"filters\": [check_char_repetition, check_flagged_words],\n        \"cleaners\": [remove_empty_lines, normalize_whitespace],\n        \"skip_global\": False,\n    },\n    # ...\n]\npipeline = Pipeline(datasources)\npipeline.run(global_filters=[minhash_dedup])\nlogger.info(f\"Final dataset size: {len(pipeline.datasources[0]['dataset'])}\")\n\nassert len(ds) &gt; len(pipeline.datasources[0][\"dataset\"])\nassert len(ds_1) &gt; len(pipeline.datasources[1][\"dataset\"])\n\n[12/02/22 05:31:30] INFO     Running datasource: wikitext                                          4230721344.py:43\n\n\n\n                    INFO     Running filter: check_char_repetition on text                         4230721344.py:17\n\n\n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n\n\n                    INFO     Running filter: check_flagged_words on text                           4230721344.py:17\n\n\n\n                    INFO     Running cleaner: remove_empty_lines on text                           4230721344.py:69\n\n\n\n[12/02/22 05:31:31] INFO     Running cleaner: normalize_whitespace on text                         4230721344.py:69\n\n\n\n                    INFO     Running datasource: elon                                              4230721344.py:43\n\n\n\n                    INFO     Running filter: check_char_repetition on text                         4230721344.py:17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[12/02/22 05:31:32] INFO     Running filter: check_flagged_words on text                           4230721344.py:17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                    INFO     Running cleaner: remove_empty_lines on text                           4230721344.py:69\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[12/02/22 05:31:33] INFO     Running cleaner: normalize_whitespace on text                         4230721344.py:69\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[12/02/22 05:31:34] INFO     Running global filter: minhash_dedup                                  4230721344.py:94\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[12/02/22 05:32:32] INFO     Final dataset size: 10557                                             1079502618.py:26\n\n\n\n\n# test the ability to skip global filters\ndatasources = [\n    {\n        \"dataset\": ds,\n        \"name\": \"wikitext\",\n        \"columns\": [\"text\"],\n        \"filters\": [check_char_repetition, check_flagged_words],\n        \"cleaners\": [remove_empty_lines, normalize_whitespace],\n        \"skip_global\": False,\n    },\n    {\n        \"dataset\": ds,\n        \"name\": \"wikitext1\",\n        \"columns\": [\"text\"],\n        \"filters\": [check_char_repetition, check_flagged_words],\n        \"cleaners\": [remove_empty_lines, normalize_whitespace],\n        \"skip_global\": True,\n    },\n    # ...\n]\npipeline = Pipeline(datasources)\npipeline.run(global_filters=global_filters)\nlogger.info(f\"Final dataset size: {len(pipeline.datasources[0]['dataset'])}\")\n\nassert len(pipeline.datasources[0][\"dataset\"]) &lt; len(pipeline.datasources[1][\"dataset\"])\n\n[12/02/22 05:33:45] INFO     Running datasource: wikitext                                          4230721344.py:43\n\n\n\n                    INFO     Running filter: check_char_repetition on text                         4230721344.py:17\n\n\n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n\n\n                    INFO     Running filter: check_flagged_words on text                           4230721344.py:17\n\n\n\n                    INFO     Running cleaner: remove_empty_lines on text                           4230721344.py:69\n\n\n\n                    INFO     Running cleaner: normalize_whitespace on text                         4230721344.py:69\n\n\n\n                    INFO     Running datasource: wikitext1                                         4230721344.py:43\n\n\n\n                    INFO     Running filter: check_char_repetition on text                         4230721344.py:17\n\n\n\n                    INFO     Running filter: check_flagged_words on text                           4230721344.py:17\n\n\n\n                    INFO     Running cleaner: remove_empty_lines on text                           4230721344.py:69\n\n\n\n                    INFO     Running cleaner: normalize_whitespace on text                         4230721344.py:69\n\n\n\n                    INFO     Running global filter: minhash_dedup                                  4230721344.py:94\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[12/02/22 05:34:29] INFO     Final dataset size: 10557                                              173363204.py:23"
  }
]